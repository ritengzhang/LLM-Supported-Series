{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install openai\n",
        "import openai\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import json\n",
        "!pip install quickchart.io\n",
        "from quickchart import QuickChart\n",
        "import re\n",
        "from IPython.display import Image, display\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import zscore, ttest_ind\n",
        "\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "97DAXFw2Ocjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46226532-49a4-4e2b-f638-ea656a0f80db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting openai\n",
            "  Downloading openai-1.38.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.38.0-py3-none-any.whl (335 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.9/335.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.38.0\n",
            "Collecting quickchart.io\n",
            "  Downloading quickchart_io-2.0.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.10/dist-packages (from quickchart.io) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->quickchart.io) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->quickchart.io) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->quickchart.io) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->quickchart.io) (2024.7.4)\n",
            "Downloading quickchart_io-2.0.0-py3-none-any.whl (5.1 kB)\n",
            "Installing collected packages: quickchart.io\n",
            "Successfully installed quickchart.io-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env OPENAI_API_KEY= xxx # use your open ai api key\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "AbXD66XQryvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ddd840-60e8-4338-f00d-403a3ffd9aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: OPENAI_API_KEY=xxx # use your open ai api key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variable for the model\n",
        "MODEL = \"gpt-4o\"\n",
        "GEN_TEMPERTURE = 0.0\n",
        "ACC_TEMPERTURE = 0.0"
      ],
      "metadata": {
        "id": "e2RiIPmnrzG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load csv"
      ],
      "metadata": {
        "id": "k_JxL7X1Sn9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the path to the CSV files\n",
        "base_path = '/content/drive/MyDrive/2024 May Eduview/sample/testing_db_id_matched/'\n",
        "\n",
        "# Initialize an empty dictionary to store the DataFrames\n",
        "dataframes = {}\n",
        "\n",
        "# Iterate over all files in the specified directory\n",
        "for file_name in os.listdir(base_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        # Extract the table name by removing the '.csv' extension\n",
        "        table_name = os.path.splitext(file_name)[0]\n",
        "        # Read the CSV file into a DataFrame\n",
        "        df = pd.read_csv(os.path.join(base_path, file_name))\n",
        "        # Store the DataFrame in the dictionary with the table name as the key\n",
        "        dataframes[table_name] = df\n",
        "\n",
        "# Display the keys of the dictionary to verify the table names\n",
        "print(dataframes.keys())\n",
        "dataframes['Students_routes']['Student_absence_days_per_year']"
      ],
      "metadata": {
        "id": "4EXTuLvikJwj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "d0675286-d55e-4846-8ff7-64867938386f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['Routes_1', 'Bus_incident', 'Trip', 'Bus_routes', 'Bus_routes_2', 'Schedules_1', 'Students_buses', 'Bus_data_1', 'Students_1', 'Students_routes', 'Schedules_2'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     5\n",
              "1     6\n",
              "2     3\n",
              "3     2\n",
              "4     0\n",
              "5     1\n",
              "6     9\n",
              "7     0\n",
              "8     1\n",
              "9    10\n",
              "Name: Student_absence_days_per_year, dtype: int64"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Student_absence_days_per_year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# predefined function"
      ],
      "metadata": {
        "id": "AdfefhFwd59t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rename_columns(df, new_column_names):\n",
        "    if new_column_names:\n",
        "        df.columns = new_column_names\n",
        "    return df\n",
        "\n",
        "def sum_columns(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].sum() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def mean_columns(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].mean() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def max_columns(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].max() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def min_columns(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].min() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def std_columns(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].std() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def var_columns(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].var() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def median_columns(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].median() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def product_columns(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].prod() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def sum_of_squares(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": (database_dict[table][column] ** 2).sum() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def mode_computation(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].mode()[0] for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def range_computation(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].max() - database_dict[table][column].min() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def sum_abs_diff_computation(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    if len(columns) != 2:\n",
        "        raise ValueError(\"Sum of absolute differences computation requires exactly 2 columns.\")\n",
        "    col1 = database_dict[columns[0][0]][columns[0][1]]\n",
        "    col2 = database_dict[columns[1][0]][columns[1][1]]\n",
        "    result = (col1 - col2).abs().sum()\n",
        "    result_df = pd.Series({'sum_abs_diff': result}, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def cumulative_sum_computation(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].cumsum() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.DataFrame(results)\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def regression(database_dict, params):\n",
        "    x_columns = params['x']\n",
        "    y_column = params['y']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    X = pd.concat([database_dict[table][column].rename(f\"col_{i}\") for i, (table, column) in enumerate(x_columns)], axis=1)\n",
        "    y = database_dict[y_column[0]][y_column[1]].rename(\"y\")\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "    predictions = model.predict(X)\n",
        "    mse = mean_squared_error(y, predictions)\n",
        "\n",
        "    coefficients = pd.Series(model.coef_, index=X.columns, name='Coefficient')\n",
        "    intercept = pd.Series({'Intercept': model.intercept_}, name='Coefficient')\n",
        "    mse_series = pd.Series({'MSE': mse}, name='Coefficient')\n",
        "    results = pd.concat([intercept, coefficients, mse_series])\n",
        "\n",
        "    result_df = results.to_frame(name=output_table_name)\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def z_score(database_dict, params):\n",
        "    column = params['column']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    data = database_dict[column[0]][column[1]]\n",
        "    z_scores = zscore(data)\n",
        "    result_df = pd.DataFrame({f'col_0': z_scores}, index=data.index)\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def t_test(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    if len(columns) != 2:\n",
        "        raise ValueError(\"T-test computation requires exactly 2 columns.\")\n",
        "    group1 = database_dict[columns[0][0]][columns[0][1]].dropna()\n",
        "    group2 = database_dict[columns[1][0]][columns[1][1]].dropna()\n",
        "    t_stat, p_value = ttest_ind(group1, group2)\n",
        "    result_df = pd.DataFrame({'T-Statistic': [t_stat], 'P-Value': [p_value]})\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def correlation(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    if len(columns) != 2:\n",
        "        raise ValueError(\"Correlation computation requires exactly 2 columns.\")\n",
        "    data1 = database_dict[columns[0][0]][columns[0][1]]\n",
        "    data2 = database_dict[columns[1][0]][columns[1][1]]\n",
        "    correlation_value = data1.corr(data2)\n",
        "    result_df = pd.DataFrame({'correlation': [correlation_value]}, index=[0])\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def skewness_computation(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].skew() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def quantile_computation(database_dict, params):\n",
        "    table, column = params['category_column']\n",
        "    quantile = params['quantile']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    data = database_dict[table][column]\n",
        "    result = data.quantile(quantile)\n",
        "    result_df = pd.Series({f'col_0': result}, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def normalization(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    data = pd.concat([database_dict[table][column] for table, column in columns], axis=1)\n",
        "    normalized_data = scaler.fit_transform(data)\n",
        "    normalized_df = pd.DataFrame(normalized_data, columns=[f\"col_{i}\" for i in range(len(columns))])\n",
        "    return rename_columns(normalized_df, output_column_names)\n",
        "\n",
        "def log_transformation(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": np.log(database_dict[table][column]) for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.DataFrame(results)\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def exp_transformation(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": np.exp(database_dict[table][column]) for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.DataFrame(results)\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def pct_change_computation(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].pct_change() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.DataFrame(results)\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def covariance_computation(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    if len(columns) != 2:\n",
        "        raise ValueError(\"Covariance computation requires exactly 2 columns.\")\n",
        "    data1 = database_dict[columns[0][0]][columns[0][1]]\n",
        "    data2 = database_dict[columns[1][0]][columns[1][1]]\n",
        "    result = data1.cov(data2)\n",
        "    result_df = pd.Series({'covariance': result}, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def iqr_computation(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].quantile(0.75) - database_dict[table][column].quantile(0.25) for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def categories_sum(database_dict, params):\n",
        "    group_by_column = params['group_by_column']\n",
        "    target_column = params['target_column']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    table = group_by_column[0]\n",
        "    group_by_col = group_by_column[1]\n",
        "    target_col = target_column[1]\n",
        "\n",
        "    grouped = database_dict[table].groupby(group_by_col)[target_col].sum()\n",
        "    result_df = grouped.reset_index(name=output_table_name)\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def categories_mean(database_dict, params):\n",
        "    group_by_column = params['group_by_column']\n",
        "    target_column = params['target_column']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    table = group_by_column[0]\n",
        "    group_by_col = group_by_column[1]\n",
        "    target_col = target_column[1]\n",
        "\n",
        "    grouped = database_dict[table].groupby(group_by_col)[target_col].mean()\n",
        "    result_df = grouped.reset_index(name=output_table_name)\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def categories_min(database_dict, params):\n",
        "    group_by_column = params['group_by_column']\n",
        "    target_column = params['target_column']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    table = group_by_column[0]\n",
        "    group_by_col = group_by_column[1]\n",
        "    target_col = target_column[1]\n",
        "\n",
        "    grouped = database_dict[table].groupby(group_by_col)[target_col].min()\n",
        "    result_df = grouped.reset_index(name=output_table_name)\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def categories_max(database_dict, params):\n",
        "    group_by_column = params['group_by_column']\n",
        "    target_column = params['target_column']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    table = group_by_column[0]\n",
        "    group_by_col = group_by_column[1]\n",
        "    target_col = target_column[1]\n",
        "\n",
        "    grouped = database_dict[table].groupby(group_by_col)[target_col].max()\n",
        "    result_df = grouped.reset_index(name=output_table_name)\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def categories_count(database_dict, params):\n",
        "    group_by_column = params['group_by_column']\n",
        "    target_column = params['target_column']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    table = group_by_column[0]\n",
        "    group_by_col = group_by_column[1]\n",
        "    target_col = target_column[1]\n",
        "\n",
        "    grouped = database_dict[table].groupby(group_by_col)[target_col].count()\n",
        "    result_df = grouped.reset_index(name=output_table_name)\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def numerical_sum(database_dict, params):\n",
        "    category_column = params['category_column']\n",
        "    target_column = params['target_column']\n",
        "    splits = params['splits']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    table = category_column[0]\n",
        "    cat_col = category_column[1]\n",
        "    target_col = target_column[1]\n",
        "\n",
        "    data = database_dict[table]\n",
        "    data['bins'] = pd.cut(data[cat_col], bins=splits)\n",
        "    grouped = data.groupby('bins')[target_col].sum().reset_index()\n",
        "    grouped.columns = ['range', 'sum']\n",
        "\n",
        "    result_df = grouped.rename(columns={'sum': output_table_name})\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def numerical_mean(database_dict, params):\n",
        "    category_column = params['category_column']\n",
        "    target_column = params['target_column']\n",
        "    splits = params['splits']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    table = category_column[0]\n",
        "    cat_col = category_column[1]\n",
        "    target_col = target_column[1]\n",
        "\n",
        "    data = database_dict[table]\n",
        "    data['bins'] = pd.cut(data[cat_col], bins=splits)\n",
        "    grouped = data.groupby('bins')[target_col].mean().reset_index()\n",
        "    grouped.columns = ['range', 'mean']\n",
        "\n",
        "    result_df = grouped.rename(columns={'mean': output_table_name})\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def count(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "    output_column_names = params['output_column_names']\n",
        "\n",
        "    results = {f\"col_{i}\": database_dict[table][column].count() for i, (table, column) in enumerate(columns)}\n",
        "    result_df = pd.Series(results, name=output_table_name).to_frame().T\n",
        "    return rename_columns(result_df, output_column_names)\n",
        "\n",
        "def group_by_columns(database_dict, params):\n",
        "    table = params['table']\n",
        "    group_by_columns = params['group_by_columns']\n",
        "    df = database_dict[table].copy()\n",
        "    return df.groupby(group_by_columns, as_index=False).apply(lambda x: x).reset_index(drop=True)\n",
        "\n",
        "def join_tables(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    how = params.get('how', 'inner')\n",
        "    output_table_name = params['output_table_name']\n",
        "\n",
        "    if len(columns) < 2:\n",
        "        raise ValueError(\"At least two columns are required for a join operation.\")\n",
        "\n",
        "    table1, column1 = columns[0]\n",
        "    result = database_dict[table1]\n",
        "\n",
        "    for table, column in columns[1:]:\n",
        "        result = result.merge(database_dict[table], left_on=column1, right_on=column, how=how)\n",
        "        column1 = column\n",
        "\n",
        "    result = result.drop_duplicates()\n",
        "    result.name = output_table_name\n",
        "\n",
        "    return result\n",
        "\n",
        "def arrange(database_dict, params):\n",
        "    columns = params['columns']\n",
        "    table_name, column_name = columns[0]\n",
        "    descending = params.get('descending', False)\n",
        "    output_table_name = params['output_table_name']\n",
        "\n",
        "    df = database_dict[table_name]\n",
        "    sorted_df = df.sort_values(by=column_name, ascending=not descending)\n",
        "    sorted_df.name = output_table_name\n",
        "\n",
        "    return sorted_df\n",
        "\n",
        "\n",
        "def filter(dataframes, params):\n",
        "    table_name = params['table']\n",
        "    column = params['column']\n",
        "    condition = params['condition']\n",
        "    output_table_name = params['output_table_name']\n",
        "\n",
        "    if table_name in dataframes:\n",
        "        df = dataframes[table_name]\n",
        "        filtered_df = df.query(f\"{column}{condition}\")\n",
        "        filtered_df.name = output_table_name\n",
        "    else:\n",
        "        print(f\"Table {table_name} not found in dataframes.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "def select_columns(dataframes, params):\n",
        "    table_name = params['table']\n",
        "    columns = params['column']\n",
        "    output_table_name = params['output_table_name']\n",
        "\n",
        "    if table_name in dataframes:\n",
        "        df = dataframes[table_name]\n",
        "        selected_df = df[columns]\n",
        "        selected_df.name = output_table_name\n",
        "    else:\n",
        "        print(f\"Table {table_name} not found in dataframes.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    return selected_df\n",
        "\n",
        "def calculate_time_difference(dataframes, params):\n",
        "    from datetime import datetime\n",
        "\n",
        "    table1 = params[\"time1\"][\"table\"]\n",
        "    column1 = params[\"time1\"][\"column\"]\n",
        "    table2 = params[\"time2\"][\"table\"]\n",
        "    column2 = params[\"time2\"][\"column\"]\n",
        "\n",
        "    times1 = dataframes[table1][column1]\n",
        "    times2 = dataframes[table2][column2]\n",
        "\n",
        "    time_differences = []\n",
        "\n",
        "    if len(times1) == len(times2):\n",
        "      for t1, t2 in zip(times1, times2):\n",
        "          # Parsing the times\n",
        "          time1 = datetime.strptime(t1, '%I:%M %p')\n",
        "          time2 = datetime.strptime(t2, '%I:%M %p')\n",
        "\n",
        "          # Calculating the difference in minutes\n",
        "          time_diff = (time1 - time2).total_seconds() / 60\n",
        "\n",
        "          # Append the time difference to the list\n",
        "          time_differences.append(time_diff)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Two columns need to have same number of rows\")\n",
        "\n",
        "    return time_differences\n",
        "def mutate(dataframes, params):\n",
        "    table_name = params['table']\n",
        "    new_columns = params['new_columns']\n",
        "    output_table_name = params['output_table_name']\n",
        "\n",
        "    if table_name in dataframes:\n",
        "        df = dataframes[table_name].copy()\n",
        "        for new_col_name, col_info in new_columns.items():\n",
        "            if isinstance(col_info, str) and col_info.startswith(\"lambda\"):\n",
        "                col_info = eval(col_info)\n",
        "                df[new_col_name] = col_info(df)\n",
        "            elif isinstance(col_info, dict):\n",
        "                for old_col_name, col_func in col_info.items():\n",
        "                    df[new_col_name] = df[old_col_name].apply(eval(col_func))\n",
        "                    df[new_col_name] = df[old_col_name].apply(eval(col_func))\n",
        "            else:\n",
        "                df[new_col_name] = col_info(df)\n",
        "\n",
        "        df.name = output_table_name\n",
        "    else:\n",
        "        print(f\"Table {table_name} not found in dataframes.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "ENv8UZFdeVyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "computations_supported = {\n",
        "    'sum': {\n",
        "        'function': sum_columns,\n",
        "        'description': 'Compute the sum of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"sum_table\", \"output_column_names\": [\"sum_column1\", \"sum_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'mean': {\n",
        "        'function': mean_columns,\n",
        "        'description': 'Compute the mean of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"mean_table\", \"output_column_names\": [\"mean_column1\", \"mean_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'max': {\n",
        "        'function': max_columns,\n",
        "        'description': 'Compute the maximum value of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"max_table\", \"output_column_names\": [\"max_column1\", \"max_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'min': {\n",
        "        'function': min_columns,\n",
        "        'description': 'Compute the minimum value of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"min_table\", \"output_column_names\": [\"min_column1\", \"min_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'std': {\n",
        "        'function': std_columns,\n",
        "        'description': 'Compute the standard deviation of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"std_table\", \"output_column_names\": [\"std_column1\", \"std_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'var': {\n",
        "        'function': var_columns,\n",
        "        'description': 'Compute the variance of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"var_table\", \"output_column_names\": [\"var_column1\", \"var_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'median': {\n",
        "        'function': median_columns,\n",
        "        'description': 'Compute the median of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"median_table\", \"output_column_names\": [\"median_column1\", \"median_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'product': {\n",
        "        'function': product_columns,\n",
        "        'description': 'Compute the product of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"product_table\", \"output_column_names\": [\"product_column1\", \"product_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'sum_of_squares': {\n",
        "        'function': sum_of_squares,\n",
        "        'description': 'Compute the sum of squares of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"sum_of_squares_table\", \"output_column_names\": [\"sum_of_squares_column1\", \"sum_of_squares_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'mode': {\n",
        "        'function': mode_computation,\n",
        "        'description': 'Compute the mode of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"mode_table\", \"output_column_names\": [\"mode_column1\", \"mode_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'range': {\n",
        "        'function': range_computation,\n",
        "        'description': 'Compute the range (max - min) of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"range_table\", \"output_column_names\": [\"range_column1\", \"range_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'sum_abs_diff': {\n",
        "        'function': sum_abs_diff_computation,\n",
        "        'description': 'Compute the sum of absolute differences between two columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"]], \"output_table_name\": \"sum_abs_diff_table\", \"output_column_names\": [\"sum_abs_diff_column1\"]}. The number of output columns should be the same as the number of input column pairs.'\n",
        "    },\n",
        "    'cumulative_sum': {\n",
        "        'function': cumulative_sum_computation,\n",
        "        'description': 'Compute the cumulative sum of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"cumulative_sum_table\", \"output_column_names\": [\"cumulative_sum_column1\", \"cumulative_sum_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'regression': {\n",
        "        'function': regression,\n",
        "        'description': 'Perform linear regression on the specified columns across tables in the database. JSON format: {\"x\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"y\": [\"table_name_y\", \"column_name_y\"], \"output_table_name\": \"regression_table\", \"output_column_names\": [\"regression_coefficient\", \"regression_intercept\"]}. The number of output columns should be two: one for the regression coefficient and one for the intercept.'\n",
        "    },\n",
        "    'z_score': {\n",
        "        'function': z_score,\n",
        "        'description': 'Compute the z-score of the specified column across tables in the database. JSON format: {\"column\": [\"table_name\", \"column_name\"], \"output_table_name\": \"z_score_table\", \"output_column_names\": [\"z_score_column\"]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    't_test': {\n",
        "        'function': t_test,\n",
        "        'description': 'Perform a t-test between two columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"]], \"output_table_name\": \"t_test_table\", \"output_column_names\": [\"t_test_statistic\", \"t_test_pvalue\"]}. The number of output columns should be two: one for the t-test statistic and one for the p-value.'\n",
        "    },\n",
        "    'correlation': {\n",
        "        'function': correlation,\n",
        "        'description': 'Compute the correlation between two columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"]], \"output_table_name\": \"correlation_table\", \"output_column_names\": [\"correlation_coefficient\"]}. The number of output columns should be one: the correlation coefficient.'\n",
        "    },\n",
        "    'skewness': {\n",
        "        'function': skewness_computation,\n",
        "        'description': 'Compute the skewness of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"skewness_table\", \"output_column_names\": [\"skewness_column1\", \"skewness_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'quantile': {\n",
        "        'function': quantile_computation,\n",
        "        'description': 'Compute the specified quantile of the specified columns across tables in the database. JSON format: {\"category_column\": [\"table_name\", \"category_column\"], \"quantile\": value, \"output_table_name\": \"quantile_table\", \"output_column_names\": [\"quantile_column\"]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'normalization': {\n",
        "        'function': normalization,\n",
        "                'description': 'Normalize the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"normalization_table\", \"output_column_names\": [\"normalization_column1\", \"normalization_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'log_transformation': {\n",
        "        'function': log_transformation,\n",
        "        'description': 'Apply log transformation to the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"log_transformation_table\", \"output_column_names\": [\"log_transformation_column1\", \"log_transformation_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'exp_transformation': {\n",
        "        'function': exp_transformation,\n",
        "        'description': 'Apply exponential transformation to the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"exp_transformation_table\", \"output_column_names\": [\"exp_transformation_column1\", \"exp_transformation_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'pct_change': {\n",
        "        'function': pct_change_computation,\n",
        "        'description': 'Compute the percentage change of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"pct_change_table\", \"output_column_names\": [\"pct_change_column1\", \"pct_change_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'covariance': {\n",
        "        'function': covariance_computation,\n",
        "        'description': 'Compute the covariance between two columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"]], \"output_table_name\": \"covariance_table\", \"output_column_names\": [\"covariance_column\"]}. The number of output columns should be one: the covariance value.'\n",
        "    },\n",
        "    'iqr': {\n",
        "        'function': iqr_computation,\n",
        "        'description': 'Compute the interquartile range (IQR) of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"iqr_table\", \"output_column_names\": [\"iqr_column1\", \"iqr_column2\", ...]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'categories_sum': {\n",
        "        'function': categories_sum,\n",
        "        'description': 'Compute the sum of one column grouped by another column in the same table. JSON format: {\"group_by_column\": [\"table_name\", \"category_column\"], \"target_column\": [\"table_name\", \"value_column\"], \"output_table_name\": \"categories_sum_table\", \"output_column_names\": [\"categories_sum_column\"]}. The number of output columns should be two: one for the group_by column and one for the summed target column.'\n",
        "    },\n",
        "    'categories_mean': {\n",
        "        'function': categories_mean,\n",
        "        'description': 'Compute the mean of one column grouped by another column in the same table. JSON format: {\"group_by_column\": [\"table_name\", \"category_column\"], \"target_column\": [\"table_name\", \"value_column\"], \"output_table_name\": \"categories_mean_table\", \"output_column_names\": [\"categories_mean_column\"]}. The number of output columns should be two: one for the group_by column and one for the mean target column.'\n",
        "    },\n",
        "    'categories_min': {\n",
        "        'function': categories_min,\n",
        "        'description': 'Compute the minimum of one column grouped by another column in the same table. JSON format: {\"group_by_column\": [\"table_name\", \"category_column\"], \"target_column\": [\"table_name\", \"value_column\"], \"output_table_name\": \"categories_min_table\", \"output_column_names\": [\"categories_min_column\"]}. The number of output columns should be two: one for the group_by column and one for the min target column.'\n",
        "    },\n",
        "    'categories_max': {\n",
        "        'function': categories_max,\n",
        "        'description': 'Compute the maximum of one column grouped by another column in the same table. JSON format: {\"group_by_column\": [\"table_name\", \"category_column\"], \"target_column\": [\"table_name\", \"value_column\"], \"output_table_name\": \"categories_max_table\", \"output_column_names\": [\"categories_max_column\"]}. The number of output columns should be two: one for the group_by column and one for the max target column.'\n",
        "    },\n",
        "    'categories_count': {\n",
        "        'function': categories_count,\n",
        "        'description': 'Compute the count of one column grouped by another column in the same table. JSON format: {\"group_by_column\": [\"table_name\", \"category_column\"], \"target_column\": [\"table_name\", \"value_column\"], \"output_table_name\": \"categories_count_table\", \"output_column_names\": [\"categories_count_column\"]}. The number of output columns should be two: one for the group_by column and one for the count target column.'\n",
        "    },\n",
        "    'numerical_sum': {\n",
        "        'function': numerical_sum,\n",
        "        'description': 'Compute the sum of one column based on numerical splits of another column in the same table. JSON format: {\"category_column\": [\"table_name\", \"category_column\"], \"target_column\": [\"table_name\", \"value_column\"], \"splits\": [split_value1, split_value2, ...], \"output_table_name\": \"numerical_sum_table\", \"output_column_names\": [\"numerical_sum_column\"]}. The number of output columns should match the number of splits plus the target column.'\n",
        "    },\n",
        "    'numerical_mean': {\n",
        "        'function': numerical_mean,\n",
        "        'description': 'Compute the mean of one column based on numerical splits of another column in the same table. JSON format: {\"category_column\": [\"table_name\", \"category_column\"], \"target_column\": [\"table_name\", \"value_column\"], \"splits\": [split_value1, split_value2, ...], \"output_table_name\": \"numerical_mean_table\", \"output_column_names\": [\"numerical_mean_column\"]}. The number of output columns should match the number of splits plus the target column.'\n",
        "    },\n",
        "    'count': {\n",
        "        'function': count,\n",
        "        'description': 'Count the number of samples of the specified columns across tables in the database. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"output_table_name\": \"count_table\", \"output_column_names\": [\"count_column\"]}. The number of output columns should be the same as the number of input columns.'\n",
        "    },\n",
        "    'Group_By': {\n",
        "            'function': group_by_columns,\n",
        "            'description': 'Group the data in a table by the specified columns. JSON format: {\"table\": \"table_name\", \"group_by_columns\": [\"column_name1\", \"column_name2\", ...]， \"output_table_name\": \"group_by_table\", \"output_column_names\": [\"group_by_column1\", \"group_by_column2\", ...]}. The number of output columns should be the same as the number of input columns. The output will be a DataFrame with the specified grouping columns included.'\n",
        "        },\n",
        "    'join_tables': {\n",
        "        'function': join_tables,\n",
        "        'description': 'Join multiple tables in the database based on the specified columns. JSON format: {\"columns\": [[\"table_name1\", \"column_name1\"], [\"table_name2\", \"column_name2\"], ...], \"how\": \"join_type\", \"output_table_name\": \"join_table\"}. The \"columns\" parameter should include at least two tables and their corresponding columns to join on. The \"how\" parameter specifies the type of join (default is \"inner\"). The output will be a DataFrame with all column names kept the same.'\n",
        "    },\n",
        "    'arrange': {\n",
        "        'function': arrange,\n",
        "        'description': 'Arrange the rows of a specified table in the database based on a given column. JSON format: {\"columns\": [[\"table_name\", \"column_name\"]], \"descending\": boolean, \"output_table_name\": \"arrange_table\"}. The \"columns\" parameter should specify the table and column to sort by. The \"descending\" parameter determines the sort order (default is ascending). The output will be a sorted DataFrame with all column names kept the same.'\n",
        "    },\n",
        "\n",
        "    'mutate' :{\n",
        "    'function': mutate,\n",
        "    'description': 'Create new columns in a specified table by applying functions to existing columns. Only include the new column name and the old column name used in the lambda function in \"new_columns\" param. JSON format: {\"table\": \"table_name\", \"new_columns\": {\"new_column_name\": {\"old_column_name\": \"lambda_function\"}}, \"output_table_name\": \"mutate_table\"}. Make sure to write the lambda_function into a string. The output will be a DataFrame with all column names kept the same.'\n",
        "    },\n",
        "\n",
        "    'filter': {\n",
        "      'function': filter,\n",
        "      'description': 'Apply filter to a specific column in a table of the database with given condition. JSON format: {\"table\": \"table_name\", \"column\": \"column_name\", \"condition\": \"condition\"}. ex. {\"table\": \"table_name\", \"column\": \"column_name\", \"condition\": \"< 5\"}. ex2. {\"table\": \"table_name\", \"column\": \"column_name\", \"condition\": \"== \"A\"\"}.'\n",
        "    },\n",
        "    'select_columns': {\n",
        "      'function': select_columns,\n",
        "      'description': 'Select specific columns from a table in the database. JSON format: {\"table\": \"table_name1\", \"column\": [\"column1\", \"column2\", ...]}.'\n",
        "    },\n",
        "    'Time Difference': {\n",
        "    'function': calculate_time_difference,\n",
        "    'description': 'Calculate the time difference between two columns of times. JSON format: {\"time1\": {\"table\": \"table1\", \"column\": \"column1\"}, \"time2\": {\"table\": \"table2\", \"column\": \"column2\"}}. The function returns a list of time differences in minutes.'\n",
        "    }\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2Ap2JkaERTzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "rfByroEnFTxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test cases\n",
        "test_cases = [\n",
        "    (\"Sum Columns\", sum_columns, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'sum_absence_experience',\n",
        "        'output_column_names': ['sum_Student_absence_days_per_year', 'sum_Experience_years']\n",
        "    }),\n",
        "    (\"Mean Columns\", mean_columns, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'mean_absence_experience',\n",
        "        'output_column_names': ['mean_Student_absence_days_per_year', 'mean_Experience_years']\n",
        "    }),\n",
        "    (\"Max Columns\", max_columns, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'max_absence_experience',\n",
        "        'output_column_names': ['max_Student_absence_days_per_year', 'max_Experience_years']\n",
        "    }),\n",
        "    (\"Min Columns\", min_columns, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'min_absence_experience',\n",
        "        'output_column_names': ['min_Student_absence_days_per_year', 'min_Experience_years']\n",
        "    }),\n",
        "    (\"Standard Deviation Columns\", std_columns, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'std_absence_experience',\n",
        "        'output_column_names': ['std_Student_absence_days_per_year', 'std_Experience_years']\n",
        "    }),\n",
        "    (\"Variance Columns\", var_columns, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'var_absence_experience',\n",
        "        'output_column_names': ['var_Student_absence_days_per_year', 'var_Experience_years']\n",
        "    }),\n",
        "    (\"Median Columns\", median_columns, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'median_absence_experience',\n",
        "        'output_column_names': ['median_Student_absence_days_per_year', 'median_Experience_years']\n",
        "    }),\n",
        "    (\"Product Columns\", product_columns, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'product_absence_experience',\n",
        "        'output_column_names': ['product_Student_absence_days_per_year', 'product_Experience_years']\n",
        "    }),\n",
        "    (\"Sum of Squares Columns\", sum_of_squares, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'sum_of_squares_absence_experience',\n",
        "        'output_column_names': ['sum_of_squares_Student_absence_days_per_year', 'sum_of_squares_Experience_years']\n",
        "    }),\n",
        "    (\"Mode Computation\", mode_computation, {\n",
        "        'columns': [['Students_routes', 'Score']],\n",
        "        'output_table_name': 'mode_score',\n",
        "        'output_column_names': ['mode_Score']\n",
        "    }),\n",
        "    (\"Range Computation\", range_computation, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'range_absence_experience',\n",
        "        'output_column_names': ['range_Student_absence_days_per_year', 'range_Experience_years']\n",
        "    }),\n",
        "    (\"Sum of Absolute Differences\", sum_abs_diff_computation, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'sum_abs_diff_absence_experience',\n",
        "        'output_column_names': ['sum_abs_diff_Student_absence_days_per_year']\n",
        "    }),\n",
        "    (\"Cumulative Sum\", cumulative_sum_computation, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year']],\n",
        "        'output_table_name': 'cumulative_sum_absence',\n",
        "        'output_column_names': ['cumulative_sum_Student_absence_days_per_year']\n",
        "    }),\n",
        "    (\"Regression\", regression, {\n",
        "        'x': [['Students_routes', 'Student_absence_days_per_year']],\n",
        "        'y': ['Students_buses', 'Experience_years'],\n",
        "        'output_table_name': 'regression_absence_experience',\n",
        "        'output_column_names': ['regression_experience']\n",
        "    }),\n",
        "    (\"Z-Score\", z_score, {\n",
        "        'column': ['Students_routes', 'Student_absence_days_per_year'],\n",
        "        'output_table_name': 'z_score_absence',\n",
        "        'output_column_names': ['z_score_Student_absence_days_per_year']\n",
        "    }),\n",
        "    (\"T-Test\", t_test, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 't_test_absence_experience',\n",
        "        'output_column_names': ['t_test_Student_absence_days_per_year', 't_test_Experience_years']\n",
        "    }),\n",
        "    (\"Correlation\", correlation, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'correlation_absence_experience',\n",
        "        'output_column_names': ['correlation_Student_absence_days_per_year']\n",
        "    }),\n",
        "    (\"Skewness Computation\", skewness_computation, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'skewness_absence_experience',\n",
        "        'output_column_names': ['skewness_Student_absence_days_per_year', 'skewness_Experience_years']\n",
        "    }),\n",
        "    (\"Quantile Computation\", quantile_computation, {\n",
        "        'category_column': ['Students_routes', 'Student_absence_days_per_year'],\n",
        "        'quantile': 0.5,\n",
        "        'output_table_name': 'quantile_absence',\n",
        "        'output_column_names': ['quantile_Student_absence_days_per_year']\n",
        "    }),\n",
        "    (\"Normalization\", normalization, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'normalization_absence_experience',\n",
        "        'output_column_names': ['normalization_Student_absence_days_per_year', 'normalization_Experience_years']\n",
        "    }),\n",
        "    (\"Log Transformation\", log_transformation, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'log_transformation_absence_experience',\n",
        "        'output_column_names': ['log_transformation_Student_absence_days_per_year', 'log_transformation_Experience_years']\n",
        "    }),\n",
        "    (\"Exponential Transformation\", exp_transformation, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'exp_transformation_absence_experience',\n",
        "        'output_column_names': ['exp_transformation_Student_absence_days_per_year', 'exp_transformation_Experience_years']\n",
        "    }),\n",
        "    (\"Percentage Change Computation\", pct_change_computation, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'pct_change_absence_experience',\n",
        "        'output_column_names': ['pct_change_Student_absence_days_per_year', 'pct_change_Experience_years']\n",
        "    }),\n",
        "    (\"Covariance Computation\", covariance_computation, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'covariance_absence_experience',\n",
        "        'output_column_names': ['covariance']\n",
        "    }),\n",
        "\n",
        "    (\"Interquartile Range Computation\", iqr_computation, {\n",
        "        'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']],\n",
        "        'output_table_name': 'iqr_absence_experience',\n",
        "        'output_column_names': ['iqr_Student_absence_days_per_year', 'iqr_Experience_years']\n",
        "    }),\n",
        "    (\"Categories Sum\", categories_sum, {\n",
        "        'group_by_column': ['Students_routes', 'Grade'],\n",
        "        'target_column': ['Students_routes', 'Student_absence_days_per_year'],\n",
        "        'output_table_name': 'categories_sum_grade_absence',\n",
        "        'output_column_names': ['categories_sum_Grade', 'categories_sum_Student_absence_days_per_year']\n",
        "    }),\n",
        "    (\"Categories Mean\", categories_mean, {\n",
        "        'group_by_column': ['Students_routes', 'Grade'],\n",
        "        'target_column': ['Students_routes', 'Student_absence_days_per_year'],\n",
        "        'output_table_name': 'categories_mean_grade_absence',\n",
        "        'output_column_names': ['categories_mean_Grade', 'categories_mean_Student_absence_days_per_year']\n",
        "    }),\n",
        "    (\"Categories Min\", categories_min, {\n",
        "        'group_by_column': ['Students_routes', 'Grade'],\n",
        "        'target_column': ['Students_routes', 'Student_absence_days_per_year'],\n",
        "        'output_table_name': 'categories_min_grade_absence',\n",
        "        'output_column_names': ['categories_min_Grade', 'categories_min_Student_absence_days_per_year']\n",
        "    }),\n",
        "    (\"Categories Max\", categories_max, {\n",
        "        'group_by_column': ['Students_routes', 'Grade'],\n",
        "        'target_column': ['Students_routes', 'Student_absence_days_per_year'],\n",
        "        'output_table_name': 'categories_max_grade_absence',\n",
        "        'output_column_names': ['categories_max_Grade', 'categories_max_Student_absence_days_per_year']\n",
        "    }),\n",
        "    (\"Categories Count\", categories_count, {\n",
        "        'group_by_column': ['Students_routes', 'Grade'],\n",
        "        'target_column': ['Students_routes', 'Student_ID'],\n",
        "        'output_table_name': 'categories_count_grade',\n",
        "        'output_column_names': ['categories_count_Grade', 'categories_count_Student_ID']\n",
        "    }),\n",
        "    (\"Numerical Sum\", numerical_sum, {\n",
        "        'category_column': ['Students_routes', 'Student_absence_days_per_year'],\n",
        "        'target_column': ['Students_routes', 'Grade'],\n",
        "        'splits': [2, 4, 6],\n",
        "        'output_table_name': 'numerical_sum_absence_grade',\n",
        "        'output_column_names': ['numerical_sum_absence', 'numerical_sum_grade']\n",
        "    }),\n",
        "    (\"Numerical Mean\", numerical_mean, {\n",
        "        'category_column': ['Students_routes', 'Student_absence_days_per_year'],\n",
        "        'target_column': ['Students_routes', 'Grade'],\n",
        "        'splits': [2, 4, 6],\n",
        "        'output_table_name': 'numerical_mean_absence_grade',\n",
        "        'output_column_names': ['numerical_mean_absence', 'numerical_mean_grade']\n",
        "    }),\n",
        "    (\"Count\", count, {\n",
        "        'columns': [['Students_routes', 'Student_ID'], ['Students_buses', 'Bus_ID']],\n",
        "        'output_table_name': 'count_student_bus',\n",
        "        'output_column_names': ['count_Student_ID', 'count_Bus_ID']\n",
        "    }),\n",
        "    (\"Group By\", group_by_columns, {\n",
        "        'table': 'Students_routes',\n",
        "        'group_by_columns': ['Gender', 'Grade'],\n",
        "        'output_table_name': 'grouped_students_routes',\n",
        "        'output_column_names': ['Gender', 'Grade', 'Count']\n",
        "    }),\n",
        "    (\"Filter\", filter, {\n",
        "        'table': 'Students_routes',\n",
        "        'column': 'Grade',\n",
        "        'condition': '== 9',\n",
        "        'output_table_name': 'filtered_students_routes'\n",
        "    }),\n",
        "    (\"Select Columns\", select_columns, {\n",
        "        'table': 'Students_routes',\n",
        "        'column': ['Student_ID', 'Grade'],\n",
        "        'output_table_name': 'selected_students_routes'\n",
        "    }),\n",
        "    (\"Join Tables\", join_tables, {\n",
        "        'columns': [['Students_routes', 'Student_ID'], ['Students_buses', 'Student_ID']],\n",
        "        'how': 'inner',\n",
        "        'output_table_name': 'joined_students_routes_buses'\n",
        "    }),\n",
        "    (\"Arrange\", arrange, {\n",
        "        'columns': [['Students_routes', 'Grade']],\n",
        "        'descending': True,\n",
        "        'output_table_name': 'arranged_students_routes'\n",
        "    }),\n",
        "    (\"Mutate\", mutate, {\n",
        "        \"table\": \"Students_routes\",\n",
        "        \"new_columns\": {\"double_grade\": \"lambda df: df['Grade'] * 2\"},\n",
        "        \"output_table_name\": \"mutated_students_routes\"\n",
        "    })\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Running the test cases\n",
        "for test_name, func, params in test_cases:\n",
        "      result = func(dataframes, params)\n",
        "      print(f\"{test_name} ({params}):\\n{result.to_string(index=False)}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZaQoDFOWFTCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b27661b-07de-465f-e40f-207d25bf0e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum Columns ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'sum_absence_experience', 'output_column_names': ['sum_Student_absence_days_per_year', 'sum_Experience_years']}):\n",
            " sum_Student_absence_days_per_year  sum_Experience_years\n",
            "                                37                    81\n",
            "\n",
            "Mean Columns ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'mean_absence_experience', 'output_column_names': ['mean_Student_absence_days_per_year', 'mean_Experience_years']}):\n",
            " mean_Student_absence_days_per_year  mean_Experience_years\n",
            "                                3.7                    8.1\n",
            "\n",
            "Max Columns ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'max_absence_experience', 'output_column_names': ['max_Student_absence_days_per_year', 'max_Experience_years']}):\n",
            " max_Student_absence_days_per_year  max_Experience_years\n",
            "                                10                    14\n",
            "\n",
            "Min Columns ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'min_absence_experience', 'output_column_names': ['min_Student_absence_days_per_year', 'min_Experience_years']}):\n",
            " min_Student_absence_days_per_year  min_Experience_years\n",
            "                                 0                     3\n",
            "\n",
            "Standard Deviation Columns ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'std_absence_experience', 'output_column_names': ['std_Student_absence_days_per_year', 'std_Experience_years']}):\n",
            " std_Student_absence_days_per_year  std_Experience_years\n",
            "                          3.653005              3.573047\n",
            "\n",
            "Variance Columns ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'var_absence_experience', 'output_column_names': ['var_Student_absence_days_per_year', 'var_Experience_years']}):\n",
            " var_Student_absence_days_per_year  var_Experience_years\n",
            "                         13.344444             12.766667\n",
            "\n",
            "Median Columns ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'median_absence_experience', 'output_column_names': ['median_Student_absence_days_per_year', 'median_Experience_years']}):\n",
            " median_Student_absence_days_per_year  median_Experience_years\n",
            "                                  2.5                      8.5\n",
            "\n",
            "Product Columns ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'product_absence_experience', 'output_column_names': ['product_Student_absence_days_per_year', 'product_Experience_years']}):\n",
            " product_Student_absence_days_per_year  product_Experience_years\n",
            "                                     0                 395176320\n",
            "\n",
            "Sum of Squares Columns ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'sum_of_squares_absence_experience', 'output_column_names': ['sum_of_squares_Student_absence_days_per_year', 'sum_of_squares_Experience_years']}):\n",
            " sum_of_squares_Student_absence_days_per_year  sum_of_squares_Experience_years\n",
            "                                          257                              771\n",
            "\n",
            "Mode Computation ({'columns': [['Students_routes', 'Score']], 'output_table_name': 'mode_score', 'output_column_names': ['mode_Score']}):\n",
            "mode_Score\n",
            "         A\n",
            "\n",
            "Range Computation ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'range_absence_experience', 'output_column_names': ['range_Student_absence_days_per_year', 'range_Experience_years']}):\n",
            " range_Student_absence_days_per_year  range_Experience_years\n",
            "                                  10                      11\n",
            "\n",
            "Sum of Absolute Differences ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'sum_abs_diff_absence_experience', 'output_column_names': ['sum_abs_diff_Student_absence_days_per_year']}):\n",
            " sum_abs_diff_Student_absence_days_per_year\n",
            "                                         50\n",
            "\n",
            "Cumulative Sum ({'columns': [['Students_routes', 'Student_absence_days_per_year']], 'output_table_name': 'cumulative_sum_absence', 'output_column_names': ['cumulative_sum_Student_absence_days_per_year']}):\n",
            " cumulative_sum_Student_absence_days_per_year\n",
            "                                            5\n",
            "                                           11\n",
            "                                           14\n",
            "                                           16\n",
            "                                           16\n",
            "                                           17\n",
            "                                           26\n",
            "                                           26\n",
            "                                           27\n",
            "                                           37\n",
            "\n",
            "Regression ({'x': [['Students_routes', 'Student_absence_days_per_year']], 'y': ['Students_buses', 'Experience_years'], 'output_table_name': 'regression_absence_experience', 'output_column_names': ['regression_experience']}):\n",
            " regression_experience\n",
            "              7.536220\n",
            "              0.152373\n",
            "             11.211157\n",
            "\n",
            "Z-Score ({'column': ['Students_routes', 'Student_absence_days_per_year'], 'output_table_name': 'z_score_absence', 'output_column_names': ['z_score_Student_absence_days_per_year']}):\n",
            " z_score_Student_absence_days_per_year\n",
            "                              0.375121\n",
            "                              0.663676\n",
            "                             -0.201988\n",
            "                             -0.490543\n",
            "                             -1.067653\n",
            "                             -0.779098\n",
            "                              1.529341\n",
            "                             -1.067653\n",
            "                             -0.779098\n",
            "                              1.817896\n",
            "\n",
            "T-Test ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 't_test_absence_experience', 'output_column_names': ['t_test_Student_absence_days_per_year', 't_test_Experience_years']}):\n",
            " t_test_Student_absence_days_per_year  t_test_Experience_years\n",
            "                            -2.722952                 0.013954\n",
            "\n",
            "Correlation ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'correlation_absence_experience', 'output_column_names': ['correlation_Student_absence_days_per_year']}):\n",
            " correlation_Student_absence_days_per_year\n",
            "                                  0.155783\n",
            "\n",
            "Skewness Computation ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'skewness_absence_experience', 'output_column_names': ['skewness_Student_absence_days_per_year', 'skewness_Experience_years']}):\n",
            " skewness_Student_absence_days_per_year  skewness_Experience_years\n",
            "                               0.761751                  -0.120207\n",
            "\n",
            "Quantile Computation ({'category_column': ['Students_routes', 'Student_absence_days_per_year'], 'quantile': 0.5, 'output_table_name': 'quantile_absence', 'output_column_names': ['quantile_Student_absence_days_per_year']}):\n",
            " quantile_Student_absence_days_per_year\n",
            "                                    2.5\n",
            "\n",
            "Normalization ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'normalization_absence_experience', 'output_column_names': ['normalization_Student_absence_days_per_year', 'normalization_Experience_years']}):\n",
            " normalization_Student_absence_days_per_year  normalization_Experience_years\n",
            "                                         0.5                        0.000000\n",
            "                                         0.6                        0.727273\n",
            "                                         0.3                        0.181818\n",
            "                                         0.2                        0.454545\n",
            "                                         0.0                        0.545455\n",
            "                                         0.1                        0.000000\n",
            "                                         0.9                        0.727273\n",
            "                                         0.0                        0.454545\n",
            "                                         0.1                        1.000000\n",
            "                                         1.0                        0.545455\n",
            "\n",
            "Log Transformation ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'log_transformation_absence_experience', 'output_column_names': ['log_transformation_Student_absence_days_per_year', 'log_transformation_Experience_years']}):\n",
            " log_transformation_Student_absence_days_per_year  log_transformation_Experience_years\n",
            "                                         1.609438                             1.098612\n",
            "                                         1.791759                             2.397895\n",
            "                                         1.098612                             1.609438\n",
            "                                         0.693147                             2.079442\n",
            "                                             -inf                             2.197225\n",
            "                                         0.000000                             1.098612\n",
            "                                         2.197225                             2.397895\n",
            "                                             -inf                             2.079442\n",
            "                                         0.000000                             2.639057\n",
            "                                         2.302585                             2.197225\n",
            "\n",
            "Exponential Transformation ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'exp_transformation_absence_experience', 'output_column_names': ['exp_transformation_Student_absence_days_per_year', 'exp_transformation_Experience_years']}):\n",
            " exp_transformation_Student_absence_days_per_year  exp_transformation_Experience_years\n",
            "                                       148.413159                         2.008554e+01\n",
            "                                       403.428793                         5.987414e+04\n",
            "                                        20.085537                         1.484132e+02\n",
            "                                         7.389056                         2.980958e+03\n",
            "                                         1.000000                         8.103084e+03\n",
            "                                         2.718282                         2.008554e+01\n",
            "                                      8103.083928                         5.987414e+04\n",
            "                                         1.000000                         2.980958e+03\n",
            "                                         2.718282                         1.202604e+06\n",
            "                                     22026.465795                         8.103084e+03\n",
            "\n",
            "Percentage Change Computation ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'pct_change_absence_experience', 'output_column_names': ['pct_change_Student_absence_days_per_year', 'pct_change_Experience_years']}):\n",
            " pct_change_Student_absence_days_per_year  pct_change_Experience_years\n",
            "                                      NaN                          NaN\n",
            "                                 0.200000                     2.666667\n",
            "                                -0.500000                    -0.545455\n",
            "                                -0.333333                     0.600000\n",
            "                                -1.000000                     0.125000\n",
            "                                      inf                    -0.666667\n",
            "                                 8.000000                     2.666667\n",
            "                                -1.000000                    -0.272727\n",
            "                                      inf                     0.750000\n",
            "                                 9.000000                    -0.357143\n",
            "\n",
            "Covariance Computation ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'covariance_absence_experience', 'output_column_names': ['covariance']}):\n",
            " covariance\n",
            "   2.033333\n",
            "\n",
            "Interquartile Range Computation ({'columns': [['Students_routes', 'Student_absence_days_per_year'], ['Students_buses', 'Experience_years']], 'output_table_name': 'iqr_absence_experience', 'output_column_names': ['iqr_Student_absence_days_per_year', 'iqr_Experience_years']}):\n",
            " iqr_Student_absence_days_per_year  iqr_Experience_years\n",
            "                              4.75                  4.75\n",
            "\n",
            "Categories Sum ({'group_by_column': ['Students_routes', 'Grade'], 'target_column': ['Students_routes', 'Student_absence_days_per_year'], 'output_table_name': 'categories_sum_grade_absence', 'output_column_names': ['categories_sum_Grade', 'categories_sum_Student_absence_days_per_year']}):\n",
            " categories_sum_Grade  categories_sum_Student_absence_days_per_year\n",
            "                    3                                             1\n",
            "                    4                                             6\n",
            "                    6                                             3\n",
            "                    9                                            21\n",
            "                   10                                             5\n",
            "                   11                                             1\n",
            "\n",
            "Categories Mean ({'group_by_column': ['Students_routes', 'Grade'], 'target_column': ['Students_routes', 'Student_absence_days_per_year'], 'output_table_name': 'categories_mean_grade_absence', 'output_column_names': ['categories_mean_Grade', 'categories_mean_Student_absence_days_per_year']}):\n",
            " categories_mean_Grade  categories_mean_Student_absence_days_per_year\n",
            "                     3                                            1.0\n",
            "                     4                                            3.0\n",
            "                     6                                            1.5\n",
            "                     9                                            7.0\n",
            "                    10                                            5.0\n",
            "                    11                                            1.0\n",
            "\n",
            "Categories Min ({'group_by_column': ['Students_routes', 'Grade'], 'target_column': ['Students_routes', 'Student_absence_days_per_year'], 'output_table_name': 'categories_min_grade_absence', 'output_column_names': ['categories_min_Grade', 'categories_min_Student_absence_days_per_year']}):\n",
            " categories_min_Grade  categories_min_Student_absence_days_per_year\n",
            "                    3                                             1\n",
            "                    4                                             0\n",
            "                    6                                             0\n",
            "                    9                                             2\n",
            "                   10                                             5\n",
            "                   11                                             1\n",
            "\n",
            "Categories Max ({'group_by_column': ['Students_routes', 'Grade'], 'target_column': ['Students_routes', 'Student_absence_days_per_year'], 'output_table_name': 'categories_max_grade_absence', 'output_column_names': ['categories_max_Grade', 'categories_max_Student_absence_days_per_year']}):\n",
            " categories_max_Grade  categories_max_Student_absence_days_per_year\n",
            "                    3                                             1\n",
            "                    4                                             6\n",
            "                    6                                             3\n",
            "                    9                                            10\n",
            "                   10                                             5\n",
            "                   11                                             1\n",
            "\n",
            "Categories Count ({'group_by_column': ['Students_routes', 'Grade'], 'target_column': ['Students_routes', 'Student_ID'], 'output_table_name': 'categories_count_grade', 'output_column_names': ['categories_count_Grade', 'categories_count_Student_ID']}):\n",
            " categories_count_Grade  categories_count_Student_ID\n",
            "                      3                            1\n",
            "                      4                            2\n",
            "                      6                            2\n",
            "                      9                            3\n",
            "                     10                            1\n",
            "                     11                            1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "<ipython-input-5-d22a157a23d1>:348: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  grouped = data.groupby('bins')[target_col].sum().reset_index()\n",
            "<ipython-input-5-d22a157a23d1>:367: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  grouped = data.groupby('bins')[target_col].mean().reset_index()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical Sum ({'category_column': ['Students_routes', 'Student_absence_days_per_year'], 'target_column': ['Students_routes', 'Grade'], 'splits': [2, 4, 6], 'output_table_name': 'numerical_sum_absence_grade', 'output_column_names': ['numerical_sum_absence', 'numerical_sum_grade']}):\n",
            "numerical_sum_absence  numerical_sum_grade\n",
            "               (2, 4]                    6\n",
            "               (4, 6]                   14\n",
            "\n",
            "Numerical Mean ({'category_column': ['Students_routes', 'Student_absence_days_per_year'], 'target_column': ['Students_routes', 'Grade'], 'splits': [2, 4, 6], 'output_table_name': 'numerical_mean_absence_grade', 'output_column_names': ['numerical_mean_absence', 'numerical_mean_grade']}):\n",
            "numerical_mean_absence  numerical_mean_grade\n",
            "                (2, 4]                   6.0\n",
            "                (4, 6]                   7.0\n",
            "\n",
            "Count ({'columns': [['Students_routes', 'Student_ID'], ['Students_buses', 'Bus_ID']], 'output_table_name': 'count_student_bus', 'output_column_names': ['count_Student_ID', 'count_Bus_ID']}):\n",
            " count_Student_ID  count_Bus_ID\n",
            "               10            10\n",
            "\n",
            "Group By ({'table': 'Students_routes', 'group_by_columns': ['Gender', 'Grade'], 'output_table_name': 'grouped_students_routes', 'output_column_names': ['Gender', 'Grade', 'Count']}):\n",
            "                          Student_ID Student_first_name Student_last_name                         Email      Gender  Grade Ethnicity          Home_address         City       State  Postal_code Phone_number       Parent_name  Siblings      Principle_name  Student_absence_days_per_year Score                             Route_ID  Route_number            Start_location              End_location Morning_arrive_time Afternoon_arrive_time  Distance(miles)  #_of_stops  #_of_intersections       bins\n",
            "91eb5c68-86ba-4ff6-99c5-acf019a3ea43               Anna            Scorer            tscorer1@nifty.com     Agender      4     White 19742 Kipling Terrace        Indio  California        18462 108-397-2676       Tobi Scorer         1 Archaimbaud Gerrans                              6     A 2b0c1e9b-b775-4ff7-8624-9dca1f47cb65           129           24 Warrior Road         26 Anzinger Court             8:03 AM               4:38 PM                8          20                  12 (4.0, 6.0]\n",
            "5aeea8d9-c5fe-4809-90eb-438858446520              Rania           Gloster          hgloster2@devhub.com    Bigender      6     White   2419 Carioca Center       Berlin Connecticut        30488 151-444-6169     Helga Gloster         2      Antony Dosdill                              3     B b0427ef9-981a-4cfd-bef3-a7dfede9d3ea           100            7322 Ryan Hill       147 Thompson Avenue             8:09 AM               4:10 PM                6          16                  10 (2.0, 4.0]\n",
            "c009d19f-b33e-443d-bd62-786417b20d6a          Gabriella         Leschelle          gleschelle4@jugem.jp      Female      4     White 13052 Stoughton Court        Macon     Georgia        42451 757-942-6066   Genni Leschelle         1       Angelika Ager                              0     A 8916198c-04c1-4186-a351-ff7b9631b5e6           102          889 Pawling Park        873 Portage Center             8:01 AM               4:11 PM                7          13                  16        NaN\n",
            "b602937c-1d8b-4bd0-8d45-b04aac150021            Chancey           Jacquot              rjacquot7@hp.com      Female      6     White 237 Autumn Leaf Point    Annapolis    Maryland        36406 999-302-7046     Rubie Jacquot         2   Crosby De Ambrosi                              0     B 455401cb-597b-45c6-9a69-ff02e3269ce1           117           3393 Fisk Point           6 Crowley Alley             8:00 AM               4:19 PM                8          13                   9        NaN\n",
            "94128f2f-1bac-4010-9c85-376e2018f835              Chloe          Draaisma          bdraaisma3@slate.com      Female      9     Asian    279 Hayes Crossing    Pensacola     Florida        21195 635-290-9734    Bonny Draaisma         1     Ancell Loughman                              2     C f2d636fe-2653-47c0-9d93-e7ba3cfc4fc8           107  32478 Declaration Center      2 Transport Junction             8:08 AM               4:51 PM                8          18                   9        NaN\n",
            "e057ec7a-3a1e-4aec-a04a-bbc848d57356              Katie         Pimblotte     kpimblotte0@indiegogo.com      Female     10    Native       7 Carberry Hill      Clifton     Arizona        12846 169-981-5634 Katinka Pimblotte         1     Georgine Najera                              5     A 10b5b456-7532-444e-9034-9c13d297a809           110          68548 6th Avenue         66758 Morrow Road             8:09 AM               4:49 PM                5          12                  12 (4.0, 6.0]\n",
            "21da79b6-29ad-4b4a-8ae5-b1f25d296330            Luciana            Mustoe       cmustoe6@soundcloud.com Genderfluid      9     White 55188 Thackeray Point   Sacramento  California        17121 563-297-6614      Carol Mustoe         2        Regina Mayne                              9     A 3884dcc2-5251-4d87-8c52-44e34b9d1e9b           107      81 Mandrake Crossing        11051 Express Lane             8:03 AM               4:37 PM                5          20                  14        NaN\n",
            "daa40a7c-6552-483e-8372-fd021ab8cf56               Noah         Silverton        gsilverton8@flickr.com        Male      3     White       865 Corscot Way East Lansing    Michigan        78920 927-363-5574  Gordan Silverton         3        Otto Bulfoot                              1     B d19d1524-5141-4960-a50d-5e1258472a8f           102 1963 Prairie Rose Terrace 50618 Brickson Park Alley             8:04 AM               4:50 PM                6          10                  18        NaN\n",
            "8390cf91-994c-4281-b95f-2cf431dc3675               Kile         MacAskill      kmacaskill5@amazon.co.uk        Male     11     Black   87 Del Sol Junction Jacksonville     Florida        79069 579-177-8729   Kalle MacAskill         3    Rutledge Musicka                              1     A bbf36647-2f0c-4754-9ac4-9efd36dc00fb           125        13273 Dwight Trail          69 Brown Parkway             8:09 AM               4:52 PM                8          20                  16        NaN\n",
            "53eecd35-6186-4231-8408-942c0d3a88d0              Alice        Darrington fdarrington9@seattletimes.com  Polygender      9     White          73 6th Point       Dillon     Montana        93822 379-143-7449  Fanya Darrington         0         Clarie Form                             10     B 985a0f36-f8a4-4221-99ad-649a9fcc6441           103         684 Morning Alley          45 Hoepker Plaza             8:07 AM               4:11 PM                8          14                   2        NaN\n",
            "\n",
            "Filter ({'table': 'Students_routes', 'column': 'Grade', 'condition': '== 9', 'output_table_name': 'filtered_students_routes'}):\n",
            "                          Student_ID Student_first_name Student_last_name                         Email      Gender  Grade Ethnicity          Home_address       City      State  Postal_code Phone_number      Parent_name  Siblings  Principle_name  Student_absence_days_per_year Score                             Route_ID  Route_number           Start_location         End_location Morning_arrive_time Afternoon_arrive_time  Distance(miles)  #_of_stops  #_of_intersections bins\n",
            "94128f2f-1bac-4010-9c85-376e2018f835              Chloe          Draaisma          bdraaisma3@slate.com      Female      9     Asian    279 Hayes Crossing  Pensacola    Florida        21195 635-290-9734   Bonny Draaisma         1 Ancell Loughman                              2     C f2d636fe-2653-47c0-9d93-e7ba3cfc4fc8           107 32478 Declaration Center 2 Transport Junction             8:08 AM               4:51 PM                8          18                   9  NaN\n",
            "21da79b6-29ad-4b4a-8ae5-b1f25d296330            Luciana            Mustoe       cmustoe6@soundcloud.com Genderfluid      9     White 55188 Thackeray Point Sacramento California        17121 563-297-6614     Carol Mustoe         2    Regina Mayne                              9     A 3884dcc2-5251-4d87-8c52-44e34b9d1e9b           107     81 Mandrake Crossing   11051 Express Lane             8:03 AM               4:37 PM                5          20                  14  NaN\n",
            "53eecd35-6186-4231-8408-942c0d3a88d0              Alice        Darrington fdarrington9@seattletimes.com  Polygender      9     White          73 6th Point     Dillon    Montana        93822 379-143-7449 Fanya Darrington         0     Clarie Form                             10     B 985a0f36-f8a4-4221-99ad-649a9fcc6441           103        684 Morning Alley     45 Hoepker Plaza             8:07 AM               4:11 PM                8          14                   2  NaN\n",
            "\n",
            "Select Columns ({'table': 'Students_routes', 'column': ['Student_ID', 'Grade'], 'output_table_name': 'selected_students_routes'}):\n",
            "                          Student_ID  Grade\n",
            "e057ec7a-3a1e-4aec-a04a-bbc848d57356     10\n",
            "91eb5c68-86ba-4ff6-99c5-acf019a3ea43      4\n",
            "5aeea8d9-c5fe-4809-90eb-438858446520      6\n",
            "94128f2f-1bac-4010-9c85-376e2018f835      9\n",
            "c009d19f-b33e-443d-bd62-786417b20d6a      4\n",
            "8390cf91-994c-4281-b95f-2cf431dc3675     11\n",
            "21da79b6-29ad-4b4a-8ae5-b1f25d296330      9\n",
            "b602937c-1d8b-4bd0-8d45-b04aac150021      6\n",
            "daa40a7c-6552-483e-8372-fd021ab8cf56      3\n",
            "53eecd35-6186-4231-8408-942c0d3a88d0      9\n",
            "\n",
            "Join Tables ({'columns': [['Students_routes', 'Student_ID'], ['Students_buses', 'Student_ID']], 'how': 'inner', 'output_table_name': 'joined_students_routes_buses'}):\n",
            "                          Student_ID Student_first_name_x Student_last_name_x                       Email_x    Gender_x  Grade_x Ethnicity_x        Home_address_x       City_x     State_x  Postal_code_x Phone_number_x     Parent_name_x  Siblings_x    Principle_name_x  Student_absence_days_per_year_x Score_x                             Route_ID  Route_number            Start_location              End_location Morning_arrive_time Afternoon_arrive_time  Distance(miles)  #_of_stops  #_of_intersections       bins Student_first_name_y Student_last_name_y                       Email_y    Gender_y  Grade_y Ethnicity_y        Home_address_y       City_y     State_y  Postal_code_y Phone_number_y     Parent_name_y  Siblings_y    Principle_name_y  Student_absence_days_per_year_y Score_y                               Bus_ID  Bus_number      Bus_make    Bus_model Maintenance_date  Bus_capacity License_plate                            Driver_ID First_name Last_name                   Email.1 Phone_number.1   Gender.1  Age  Experience_years\n",
            "e057ec7a-3a1e-4aec-a04a-bbc848d57356                Katie           Pimblotte     kpimblotte0@indiegogo.com      Female       10      Native       7 Carberry Hill      Clifton     Arizona          12846   169-981-5634 Katinka Pimblotte           1     Georgine Najera                                5       A 10b5b456-7532-444e-9034-9c13d297a809           110          68548 6th Avenue         66758 Morrow Road             8:09 AM               4:49 PM                5          12                  12 (4.0, 6.0]                Katie           Pimblotte     kpimblotte0@indiegogo.com      Female       10      Native       7 Carberry Hill      Clifton     Arizona          12846   169-981-5634 Katinka Pimblotte           1     Georgine Najera                                5       A 01d3890d-c6ac-4792-9235-a2f6fb1cec81         142      Plymouth   Colt Vista        5/18/2024            54        L48647 1339f5fb-5eb6-4120-9cff-356a0ce54702      Tarra  Blissett       tblissett0@etsy.com   375-315-4789     Female   65                 3\n",
            "91eb5c68-86ba-4ff6-99c5-acf019a3ea43                 Anna              Scorer            tscorer1@nifty.com     Agender        4       White 19742 Kipling Terrace        Indio  California          18462   108-397-2676       Tobi Scorer           1 Archaimbaud Gerrans                                6       A 2b0c1e9b-b775-4ff7-8624-9dca1f47cb65           129           24 Warrior Road         26 Anzinger Court             8:03 AM               4:38 PM                8          20                  12 (4.0, 6.0]                 Anna              Scorer            tscorer1@nifty.com     Agender        4       White 19742 Kipling Terrace        Indio  California          18462   108-397-2676       Tobi Scorer           1 Archaimbaud Gerrans                                6       A bd6c6800-ed63-4891-afc4-1da7afdcbf29         117     Chevrolet     Cavalier        2/13/2024            52        L38733 f4b19c7b-2dab-4004-9c44-6ddae3cb850c   Salvador    Pepper spepper1@surveymonkey.com   851-576-0798 Non-binary   56                11\n",
            "5aeea8d9-c5fe-4809-90eb-438858446520                Rania             Gloster          hgloster2@devhub.com    Bigender        6       White   2419 Carioca Center       Berlin Connecticut          30488   151-444-6169     Helga Gloster           2      Antony Dosdill                                3       B b0427ef9-981a-4cfd-bef3-a7dfede9d3ea           100            7322 Ryan Hill       147 Thompson Avenue             8:09 AM               4:10 PM                6          16                  10 (2.0, 4.0]                Rania             Gloster          hgloster2@devhub.com    Bigender        6       White   2419 Carioca Center       Berlin Connecticut          30488   151-444-6169     Helga Gloster           2      Antony Dosdill                                3       B 45069a00-9dc4-4bb9-8dd0-d3b20fee131c         128         Mazda       Mazda3        9/29/2023            48        L76225 5ab1598b-ae3f-41fc-b40f-32880018e671   Germaine  Longhorn   glonghorn2@mapquest.com   266-228-6986       Male   46                 5\n",
            "94128f2f-1bac-4010-9c85-376e2018f835                Chloe            Draaisma          bdraaisma3@slate.com      Female        9       Asian    279 Hayes Crossing    Pensacola     Florida          21195   635-290-9734    Bonny Draaisma           1     Ancell Loughman                                2       C f2d636fe-2653-47c0-9d93-e7ba3cfc4fc8           107  32478 Declaration Center      2 Transport Junction             8:08 AM               4:51 PM                8          18                   9        NaN                Chloe            Draaisma          bdraaisma3@slate.com      Female        9       Asian    279 Hayes Crossing    Pensacola     Florida          21195   635-290-9734    Bonny Draaisma           1     Ancell Loughman                                2       C b21c2625-1d0b-43cb-85a7-a501a988d4f8         142         Isuzu      Trooper        5/13/2024            60        L16978 7fe40451-0671-4960-9607-06da615bd74c      Daisy   Dickins         ddickins3@mit.edu   557-503-3954     Female   61                 8\n",
            "c009d19f-b33e-443d-bd62-786417b20d6a            Gabriella           Leschelle          gleschelle4@jugem.jp      Female        4       White 13052 Stoughton Court        Macon     Georgia          42451   757-942-6066   Genni Leschelle           1       Angelika Ager                                0       A 8916198c-04c1-4186-a351-ff7b9631b5e6           102          889 Pawling Park        873 Portage Center             8:01 AM               4:11 PM                7          13                  16        NaN            Gabriella           Leschelle          gleschelle4@jugem.jp      Female        4       White 13052 Stoughton Court        Macon     Georgia          42451   757-942-6066   Genni Leschelle           1       Angelika Ager                                0       A f03e7690-cb91-4b37-8cab-2ffc979810f9         145     Chevrolet     Suburban       12/22/2023            50        L43392 eaa14df3-6f3e-4e8d-95bc-758bd838488d  Madelaine  Salvador       msalvador4@imdb.com   686-426-1699     Female   44                 9\n",
            "8390cf91-994c-4281-b95f-2cf431dc3675                 Kile           MacAskill      kmacaskill5@amazon.co.uk        Male       11       Black   87 Del Sol Junction Jacksonville     Florida          79069   579-177-8729   Kalle MacAskill           3    Rutledge Musicka                                1       A bbf36647-2f0c-4754-9ac4-9efd36dc00fb           125        13273 Dwight Trail          69 Brown Parkway             8:09 AM               4:52 PM                8          20                  16        NaN                 Kile           MacAskill      kmacaskill5@amazon.co.uk        Male       11       Black   87 Del Sol Junction Jacksonville     Florida          79069   579-177-8729   Kalle MacAskill           3    Rutledge Musicka                                1       A baed0671-b627-4e23-b209-14348382a13f         114         Dodge     Ram 2500        3/18/2024            57        L84367 d9bec0b8-6def-4492-a5a2-8c1f6609f71f    Bradley     Rains         brains5@house.gov   604-284-1551       Male   60                 3\n",
            "21da79b6-29ad-4b4a-8ae5-b1f25d296330              Luciana              Mustoe       cmustoe6@soundcloud.com Genderfluid        9       White 55188 Thackeray Point   Sacramento  California          17121   563-297-6614      Carol Mustoe           2        Regina Mayne                                9       A 3884dcc2-5251-4d87-8c52-44e34b9d1e9b           107      81 Mandrake Crossing        11051 Express Lane             8:03 AM               4:37 PM                5          20                  14        NaN              Luciana              Mustoe       cmustoe6@soundcloud.com Genderfluid        9       White 55188 Thackeray Point   Sacramento  California          17121   563-297-6614      Carol Mustoe           2        Regina Mayne                                9       A 83333481-f973-470b-9ca3-b91db423696f         106         Honda      del Sol         2/5/2024            51        L43777 50ace7de-3834-4669-80c0-b6bd96e400e2      Janey     Urien      jurien6@illinois.edu   563-947-4640     Female   52                11\n",
            "b602937c-1d8b-4bd0-8d45-b04aac150021              Chancey             Jacquot              rjacquot7@hp.com      Female        6       White 237 Autumn Leaf Point    Annapolis    Maryland          36406   999-302-7046     Rubie Jacquot           2   Crosby De Ambrosi                                0       B 455401cb-597b-45c6-9a69-ff02e3269ce1           117           3393 Fisk Point           6 Crowley Alley             8:00 AM               4:19 PM                8          13                   9        NaN              Chancey             Jacquot              rjacquot7@hp.com      Female        6       White 237 Autumn Leaf Point    Annapolis    Maryland          36406   999-302-7046     Rubie Jacquot           2   Crosby De Ambrosi                                0       B 6217d9a8-d33c-4af8-99fc-1a26700abb31         116 Mercedes-Benz    CLS-Class        7/13/2023            49        L88513 119e9a76-c8f5-44fd-821a-0745a3694961     Jeffry   Durrett jdurrett7@squarespace.com   318-739-5408       Male   40                 8\n",
            "daa40a7c-6552-483e-8372-fd021ab8cf56                 Noah           Silverton        gsilverton8@flickr.com        Male        3       White       865 Corscot Way East Lansing    Michigan          78920   927-363-5574  Gordan Silverton           3        Otto Bulfoot                                1       B d19d1524-5141-4960-a50d-5e1258472a8f           102 1963 Prairie Rose Terrace 50618 Brickson Park Alley             8:04 AM               4:50 PM                6          10                  18        NaN                 Noah           Silverton        gsilverton8@flickr.com        Male        3       White       865 Corscot Way East Lansing    Michigan          78920   927-363-5574  Gordan Silverton           3        Otto Bulfoot                                1       B fb213e07-3652-4e2a-a05a-7d1391af22a6         136    Volkswagen          Fox       12/21/2023            58        L81946 cd3d7d05-854e-4a1d-9a3e-e093d08ee568    Terrill    Bluett      tbluett8@twitpic.com   397-636-2379       Male   41                14\n",
            "53eecd35-6186-4231-8408-942c0d3a88d0                Alice          Darrington fdarrington9@seattletimes.com  Polygender        9       White          73 6th Point       Dillon     Montana          93822   379-143-7449  Fanya Darrington           0         Clarie Form                               10       B 985a0f36-f8a4-4221-99ad-649a9fcc6441           103         684 Morning Alley          45 Hoepker Plaza             8:07 AM               4:11 PM                8          14                   2        NaN                Alice          Darrington fdarrington9@seattletimes.com  Polygender        9       White          73 6th Point       Dillon     Montana          93822   379-143-7449  Fanya Darrington           0         Clarie Form                               10       B 8eaca5d6-568f-4208-8e1e-2b27d0e0d069         134     Chevrolet Express 2500         9/2/2023            58        L79307 df372119-8f2c-45b3-8a13-a550e1b61049   Anabelle    Cowely        acowely9@goo.ne.jp   948-913-2891     Female   52                 9\n",
            "\n",
            "Arrange ({'columns': [['Students_routes', 'Grade']], 'descending': True, 'output_table_name': 'arranged_students_routes'}):\n",
            "                          Student_ID Student_first_name Student_last_name                         Email      Gender  Grade Ethnicity          Home_address         City       State  Postal_code Phone_number       Parent_name  Siblings      Principle_name  Student_absence_days_per_year Score                             Route_ID  Route_number            Start_location              End_location Morning_arrive_time Afternoon_arrive_time  Distance(miles)  #_of_stops  #_of_intersections       bins\n",
            "8390cf91-994c-4281-b95f-2cf431dc3675               Kile         MacAskill      kmacaskill5@amazon.co.uk        Male     11     Black   87 Del Sol Junction Jacksonville     Florida        79069 579-177-8729   Kalle MacAskill         3    Rutledge Musicka                              1     A bbf36647-2f0c-4754-9ac4-9efd36dc00fb           125        13273 Dwight Trail          69 Brown Parkway             8:09 AM               4:52 PM                8          20                  16        NaN\n",
            "e057ec7a-3a1e-4aec-a04a-bbc848d57356              Katie         Pimblotte     kpimblotte0@indiegogo.com      Female     10    Native       7 Carberry Hill      Clifton     Arizona        12846 169-981-5634 Katinka Pimblotte         1     Georgine Najera                              5     A 10b5b456-7532-444e-9034-9c13d297a809           110          68548 6th Avenue         66758 Morrow Road             8:09 AM               4:49 PM                5          12                  12 (4.0, 6.0]\n",
            "94128f2f-1bac-4010-9c85-376e2018f835              Chloe          Draaisma          bdraaisma3@slate.com      Female      9     Asian    279 Hayes Crossing    Pensacola     Florida        21195 635-290-9734    Bonny Draaisma         1     Ancell Loughman                              2     C f2d636fe-2653-47c0-9d93-e7ba3cfc4fc8           107  32478 Declaration Center      2 Transport Junction             8:08 AM               4:51 PM                8          18                   9        NaN\n",
            "21da79b6-29ad-4b4a-8ae5-b1f25d296330            Luciana            Mustoe       cmustoe6@soundcloud.com Genderfluid      9     White 55188 Thackeray Point   Sacramento  California        17121 563-297-6614      Carol Mustoe         2        Regina Mayne                              9     A 3884dcc2-5251-4d87-8c52-44e34b9d1e9b           107      81 Mandrake Crossing        11051 Express Lane             8:03 AM               4:37 PM                5          20                  14        NaN\n",
            "53eecd35-6186-4231-8408-942c0d3a88d0              Alice        Darrington fdarrington9@seattletimes.com  Polygender      9     White          73 6th Point       Dillon     Montana        93822 379-143-7449  Fanya Darrington         0         Clarie Form                             10     B 985a0f36-f8a4-4221-99ad-649a9fcc6441           103         684 Morning Alley          45 Hoepker Plaza             8:07 AM               4:11 PM                8          14                   2        NaN\n",
            "5aeea8d9-c5fe-4809-90eb-438858446520              Rania           Gloster          hgloster2@devhub.com    Bigender      6     White   2419 Carioca Center       Berlin Connecticut        30488 151-444-6169     Helga Gloster         2      Antony Dosdill                              3     B b0427ef9-981a-4cfd-bef3-a7dfede9d3ea           100            7322 Ryan Hill       147 Thompson Avenue             8:09 AM               4:10 PM                6          16                  10 (2.0, 4.0]\n",
            "b602937c-1d8b-4bd0-8d45-b04aac150021            Chancey           Jacquot              rjacquot7@hp.com      Female      6     White 237 Autumn Leaf Point    Annapolis    Maryland        36406 999-302-7046     Rubie Jacquot         2   Crosby De Ambrosi                              0     B 455401cb-597b-45c6-9a69-ff02e3269ce1           117           3393 Fisk Point           6 Crowley Alley             8:00 AM               4:19 PM                8          13                   9        NaN\n",
            "91eb5c68-86ba-4ff6-99c5-acf019a3ea43               Anna            Scorer            tscorer1@nifty.com     Agender      4     White 19742 Kipling Terrace        Indio  California        18462 108-397-2676       Tobi Scorer         1 Archaimbaud Gerrans                              6     A 2b0c1e9b-b775-4ff7-8624-9dca1f47cb65           129           24 Warrior Road         26 Anzinger Court             8:03 AM               4:38 PM                8          20                  12 (4.0, 6.0]\n",
            "c009d19f-b33e-443d-bd62-786417b20d6a          Gabriella         Leschelle          gleschelle4@jugem.jp      Female      4     White 13052 Stoughton Court        Macon     Georgia        42451 757-942-6066   Genni Leschelle         1       Angelika Ager                              0     A 8916198c-04c1-4186-a351-ff7b9631b5e6           102          889 Pawling Park        873 Portage Center             8:01 AM               4:11 PM                7          13                  16        NaN\n",
            "daa40a7c-6552-483e-8372-fd021ab8cf56               Noah         Silverton        gsilverton8@flickr.com        Male      3     White       865 Corscot Way East Lansing    Michigan        78920 927-363-5574  Gordan Silverton         3        Otto Bulfoot                              1     B d19d1524-5141-4960-a50d-5e1258472a8f           102 1963 Prairie Rose Terrace 50618 Brickson Park Alley             8:04 AM               4:50 PM                6          10                  18        NaN\n",
            "\n",
            "Mutate ({'table': 'Students_routes', 'new_columns': {'double_grade': \"lambda df: df['Grade'] * 2\"}, 'output_table_name': 'mutated_students_routes'}):\n",
            "                          Student_ID Student_first_name Student_last_name                         Email      Gender  Grade Ethnicity          Home_address         City       State  Postal_code Phone_number       Parent_name  Siblings      Principle_name  Student_absence_days_per_year Score                             Route_ID  Route_number            Start_location              End_location Morning_arrive_time Afternoon_arrive_time  Distance(miles)  #_of_stops  #_of_intersections       bins  double_grade\n",
            "e057ec7a-3a1e-4aec-a04a-bbc848d57356              Katie         Pimblotte     kpimblotte0@indiegogo.com      Female     10    Native       7 Carberry Hill      Clifton     Arizona        12846 169-981-5634 Katinka Pimblotte         1     Georgine Najera                              5     A 10b5b456-7532-444e-9034-9c13d297a809           110          68548 6th Avenue         66758 Morrow Road             8:09 AM               4:49 PM                5          12                  12 (4.0, 6.0]            20\n",
            "91eb5c68-86ba-4ff6-99c5-acf019a3ea43               Anna            Scorer            tscorer1@nifty.com     Agender      4     White 19742 Kipling Terrace        Indio  California        18462 108-397-2676       Tobi Scorer         1 Archaimbaud Gerrans                              6     A 2b0c1e9b-b775-4ff7-8624-9dca1f47cb65           129           24 Warrior Road         26 Anzinger Court             8:03 AM               4:38 PM                8          20                  12 (4.0, 6.0]             8\n",
            "5aeea8d9-c5fe-4809-90eb-438858446520              Rania           Gloster          hgloster2@devhub.com    Bigender      6     White   2419 Carioca Center       Berlin Connecticut        30488 151-444-6169     Helga Gloster         2      Antony Dosdill                              3     B b0427ef9-981a-4cfd-bef3-a7dfede9d3ea           100            7322 Ryan Hill       147 Thompson Avenue             8:09 AM               4:10 PM                6          16                  10 (2.0, 4.0]            12\n",
            "94128f2f-1bac-4010-9c85-376e2018f835              Chloe          Draaisma          bdraaisma3@slate.com      Female      9     Asian    279 Hayes Crossing    Pensacola     Florida        21195 635-290-9734    Bonny Draaisma         1     Ancell Loughman                              2     C f2d636fe-2653-47c0-9d93-e7ba3cfc4fc8           107  32478 Declaration Center      2 Transport Junction             8:08 AM               4:51 PM                8          18                   9        NaN            18\n",
            "c009d19f-b33e-443d-bd62-786417b20d6a          Gabriella         Leschelle          gleschelle4@jugem.jp      Female      4     White 13052 Stoughton Court        Macon     Georgia        42451 757-942-6066   Genni Leschelle         1       Angelika Ager                              0     A 8916198c-04c1-4186-a351-ff7b9631b5e6           102          889 Pawling Park        873 Portage Center             8:01 AM               4:11 PM                7          13                  16        NaN             8\n",
            "8390cf91-994c-4281-b95f-2cf431dc3675               Kile         MacAskill      kmacaskill5@amazon.co.uk        Male     11     Black   87 Del Sol Junction Jacksonville     Florida        79069 579-177-8729   Kalle MacAskill         3    Rutledge Musicka                              1     A bbf36647-2f0c-4754-9ac4-9efd36dc00fb           125        13273 Dwight Trail          69 Brown Parkway             8:09 AM               4:52 PM                8          20                  16        NaN            22\n",
            "21da79b6-29ad-4b4a-8ae5-b1f25d296330            Luciana            Mustoe       cmustoe6@soundcloud.com Genderfluid      9     White 55188 Thackeray Point   Sacramento  California        17121 563-297-6614      Carol Mustoe         2        Regina Mayne                              9     A 3884dcc2-5251-4d87-8c52-44e34b9d1e9b           107      81 Mandrake Crossing        11051 Express Lane             8:03 AM               4:37 PM                5          20                  14        NaN            18\n",
            "b602937c-1d8b-4bd0-8d45-b04aac150021            Chancey           Jacquot              rjacquot7@hp.com      Female      6     White 237 Autumn Leaf Point    Annapolis    Maryland        36406 999-302-7046     Rubie Jacquot         2   Crosby De Ambrosi                              0     B 455401cb-597b-45c6-9a69-ff02e3269ce1           117           3393 Fisk Point           6 Crowley Alley             8:00 AM               4:19 PM                8          13                   9        NaN            12\n",
            "daa40a7c-6552-483e-8372-fd021ab8cf56               Noah         Silverton        gsilverton8@flickr.com        Male      3     White       865 Corscot Way East Lansing    Michigan        78920 927-363-5574  Gordan Silverton         3        Otto Bulfoot                              1     B d19d1524-5141-4960-a50d-5e1258472a8f           102 1963 Prairie Rose Terrace 50618 Brickson Park Alley             8:04 AM               4:50 PM                6          10                  18        NaN             6\n",
            "53eecd35-6186-4231-8408-942c0d3a88d0              Alice        Darrington fdarrington9@seattletimes.com  Polygender      9     White          73 6th Point       Dillon     Montana        93822 379-143-7449  Fanya Darrington         0         Clarie Form                             10     B 985a0f36-f8a4-4221-99ad-649a9fcc6441           103         684 Morning Alley          45 Hoepker Plaza             8:07 AM               4:11 PM                8          14                   2        NaN            18\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbots"
      ],
      "metadata": {
        "id": "gxe5F2puaxLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "schema for our testing database"
      ],
      "metadata": {
        "id": "-4t3jaNgUJhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = \"\"\"\n",
        "Schema for School Transportation System:\n",
        "\n",
        "1. Students_routes:\n",
        "    - Student_ID: STRING, Unique\n",
        "    - Student_first_name: STRING\n",
        "    - Student_last_name: STRING\n",
        "    - Email: STRING\n",
        "    - Gender: STRING, Values ['Male', 'Female', 'Genderqueer', 'Non-binary', 'Agender', 'Genderfluid', 'Bigender', 'Polygender']\n",
        "    - Grade: INTEGER, Range [1, 12]\n",
        "    - Ethnicity: STRING, Values ['Asian', 'White', 'Black', 'Native']\n",
        "    - Home_address: STRING\n",
        "    - City: STRING\n",
        "    - State: STRING\n",
        "    - Postal_code: INTEGER, Range [10000, 99999]\n",
        "    - Phone_number: STRING, Unique, Pattern 'XXX-XXX-XXXX'\n",
        "    - Parent_name: STRING\n",
        "    - Siblings: INTEGER, Range [0, 5]\n",
        "    - Principle_name: STRING\n",
        "    - Student_absence_days_per_year: INTEGER, Range [1, 10]\n",
        "    - Score: STRING, Values ['A', 'B', 'C']\n",
        "    - Route_ID: STRING, Unique\n",
        "    - Route_number: INTEGER, Range [100, 130]\n",
        "    - Start_location: STRING\n",
        "    - End_location: STRING\n",
        "    - Morning_arrive_time: STRING, Pattern 'HH:MM AM\n",
        "    - Afternoon_arrive_time: STRING, Pattern 'HH:MM PM\n",
        "    - Distance(miles): INTEGER, Range [5, 8]\n",
        "    - #_of_stops: INTEGER, Range [10, 20]\n",
        "    - #_of_intersections: INTEGER, Range [1, 20]\n",
        "\n",
        "2. Students_buses:\n",
        "    - Student_ID: STRING, Unique\n",
        "    - Student_first_name: STRING\n",
        "    - Student_last_name: STRING\n",
        "    - Email: STRING\n",
        "    - Gender: STRING, Values ['Male', 'Female', 'Genderqueer', 'Non-binary', 'Agender', 'Genderfluid', 'Bigender', 'Polygender']\n",
        "    - Grade: INTEGER, Range [1, 12]\n",
        "    - Ethnicity: STRING, Values ['Asian', 'White', 'Black', 'Native']\n",
        "    - Home_address: STRING\n",
        "    - City: STRING\n",
        "    - State: STRING\n",
        "    - Postal_code: INTEGER, Range [10000, 99999]\n",
        "    - Phone_number: STRING, Unique, Pattern 'XXX-XXX-XXXX'\n",
        "    - Parent_name: STRING\n",
        "    - Siblings: INTEGER, Range [0, 5]\n",
        "    - Principle_name: STRING\n",
        "    - Student_absence_days_per_year: INTEGER, Range [1, 10]\n",
        "    - Score: STRING, Values ['A', 'B', 'C']\n",
        "    - Bus_ID: STRING, Unique\n",
        "    - Bus_number: INTEGER, Unique, Range [100, 199]\n",
        "    - Bus_make: STRING\n",
        "    - Bus_model: STRING\n",
        "    - Maintenac_date: STRING, Pattern 'XX/XX/202X'\n",
        "    - Bus_capacity: INTEGER, Range [48, 60]\n",
        "    - License_plate: STRING, Unique, Pattern 'LXXXXX'\n",
        "    - Driver_ID: STRING, Unique\n",
        "    - First_name: STRING\n",
        "    - Last_name: STRING\n",
        "    - Email: STRING\n",
        "    - Phone_number: STRING, Unique, Pattern 'XXX-XXX-XXXX'\n",
        "    - Gender: STRING, Values ['Male', 'Female', 'Genderqueer', 'Non-binary', 'Agender', 'Genderfluid', 'Bigender', 'Polygender']\n",
        "    - Age: INTEGER, Range [35, 65]\n",
        "    - Experience_years: INTEGER, Range [1, 15]\n",
        "\n",
        "3. Students_1:\n",
        "    - Student_ID: STRING, Unique\n",
        "    - Student_first_name: STRING\n",
        "    - Student_last_name: STRING\n",
        "    - Email: STRING\n",
        "    - Gender: STRING, Values ['Male', 'Female', 'Genderqueer', 'Non-binary', 'Agender', 'Genderfluid', 'Bigender', 'Polygender']\n",
        "    - Grade: INTEGER, Range [1, 12]\n",
        "    - Ethnicity: STRING, Values ['Asian', 'White', 'Black', 'Native']\n",
        "    - Home_address: STRING\n",
        "    - City: STRING\n",
        "    - State: STRING\n",
        "    - Postal_code: INTEGER, Range [10000, 99999]\n",
        "    - Phone_number: STRING, Unique, Pattern 'XXX-XXX-XXXX'\n",
        "    - Parent_name: STRING\n",
        "    - Siblings: INTEGER, Range [0, 5]\n",
        "    - Principle_name: STRING\n",
        "    - Student_absence_days_per_year: INTEGER, Range [1, 10]\n",
        "    - Score: STRING, Values ['A', 'B', 'C']\n",
        "\n",
        "4. Schedules_2:\n",
        "    - Schedule_ID: STRING, Unique\n",
        "    - Route_ID: STRING, Unique\n",
        "    - Driver_ID: STRING, Unique\n",
        "    - Bus_number: INTEGER, Unique, Range [100, 199]\n",
        "    - Bus_ID: STRING, Unique\n",
        "    - Morning_arrive_time: STRING, Pattern 'HH:MM AM\n",
        "    - Afternoon_arrive_time: STRING, Pattern 'HH:MM PM\n",
        "    - Start_location: STRING\n",
        "    - End_location: STRING\n",
        "\n",
        "5. Schedules_1:\n",
        "    - Schedule_ID: STRING, Unique\n",
        "    - Route_ID: STRING, Unique\n",
        "    - Driver_ID: STRING, Unique\n",
        "    - Bus_number: INTEGER, Unique, Range [100, 199]\n",
        "    - Bus_ID: STRING, Unique\n",
        "    - Morning_arrive_time: STRING, Pattern 'HH:MM AM\n",
        "    - Afternoon_arrive_time: STRING, Pattern 'HH:MM PM\n",
        "    - Start_location: STRING\n",
        "    - End_location: STRING\n",
        "\n",
        "6. Routes_1:\n",
        "    - Route_ID: STRING, Unique\n",
        "    - Route_number: INTEGER, Range [100, 130]\n",
        "    - Start_location: STRING\n",
        "    - End_location: STRING\n",
        "    - Morning_arrive_time: STRING, Pattern 'HH:MM AM\n",
        "    - Afternoon_arrive_time: STRING, Pattern 'HH:MM PM\n",
        "    - Distance(miles): INTEGER, Range [5, 8]\n",
        "    - #_of_stops: INTEGER, Range [10, 20]\n",
        "    - #_of_intersections: INTEGER, Range [1, 20]\n",
        "\n",
        "7. Bus_routes:\n",
        "    - Route_ID: STRING, Unique\n",
        "    - Route_number: INTEGER, Range [100, 130]\n",
        "    - Start_location: STRING\n",
        "    - End_location: STRING\n",
        "    - Morning_arrive_time: STRING, Pattern 'HH:MM AM\n",
        "    - Afternoon_arrive_time: STRING, Pattern 'HH:MM PM\n",
        "    - Distance(miles): INTEGER, Range [5, 8]\n",
        "    - #_of_stops: INTEGER, Range [10, 20]\n",
        "    - #_of_intersections: INTEGER, Range [1, 20]\n",
        "    - Bus_ID: STRING, Unique\n",
        "    - Bus_number: INTEGER, Unique, Range [100, 199]\n",
        "    - Bus_make: STRING\n",
        "    - Bus_model: STRING\n",
        "    - Maintenance_date: STRING, Pattern 'XX/XX/202X'\n",
        "    - Bus_capacity: INTEGER, Range [48, 60]\n",
        "    - License_plate: STRING, Unique, Pattern 'LXXXXX'\n",
        "    - Driver_ID: STRING, Unique\n",
        "    - First_name: STRING\n",
        "    - Last_name: STRING\n",
        "    - Email: STRING\n",
        "    - Phone_number: STRING, Unique, Pattern 'XXX-XXX-XXXX'\n",
        "    - Gender: STRING, Values ['Male', 'Female', 'Genderqueer', 'Non-binary', 'Agender', 'Genderfluid', 'Bigender', 'Polygender']\n",
        "    - Age: INTEGER, Range [35, 65]\n",
        "    - Experience_years: INTEGER, Range [1, 15]\n",
        "    - On_schedule_rate(%): INTEGER, Range [70, 100]\n",
        "    - Driver_absence_days_per_year: INTEGER, Range [0, 10]\n",
        "\n",
        "8. Bus_routes_2:\n",
        "    - Route_ID: STRING, Unique\n",
        "    - Route_number: INTEGER, Range [100, 130]\n",
        "    - Start_location: STRING\n",
        "    - End_location: STRING\n",
        "    - Morning_arrive_time: STRING, Pattern 'HH:MM AM\n",
        "    - Afternoon_arrive_time: STRING, Pattern 'HH:MM PM\n",
        "    - Distance(Miles): INTEGER, Range [5, 8]\n",
        "    - #_of_stops: INTEGER, Range [10, 20]\n",
        "    - #_of_intersections: INTEGER, Range [1, 20]\n",
        "    - Bus_ID: STRING, Unique\n",
        "    - Bus_number: INTEGER, Unique, Range [100, 199]\n",
        "    - Bus_make: STRING\n",
        "    - Bus_model: STRING\n",
        "    - Maintenac_date: STRING, Pattern 'XX/XX/202X'\n",
        "    - Bus_capacity: INTEGER, Range [48, 60]\n",
        "    - License_plate: STRING, Unique, Pattern 'LXXXXX'\n",
        "    - Driver_ID: STRING, Unique\n",
        "    - First_name: STRING\n",
        "    - Last_name: STRING\n",
        "    - Email: STRING\n",
        "    - Phone_number: STRING, Unique, Pattern 'XXX-XXX-XXXX'\n",
        "    - Gender: STRING, Values ['Male', 'Female', 'Genderqueer', 'Non-binary', 'Agender', 'Genderfluid', 'Bigender', 'Polygender']\n",
        "    - Age: INTEGER, Range [35, 65]\n",
        "    - Experience_years: INTEGER, Range [1, 15]\n",
        "    - On_schedule_rate(%): INTEGER, Range [70, 100]\n",
        "    - Driver_absence_days_per_year: INTEGER, Range [0, 10]\n",
        "\n",
        "9. Bus_incident:\n",
        "    - School_year: STRING, Pattern '20XX-20XX'\n",
        "    - Bus_breakdown_ID: STRING, Unique\n",
        "    - Run_type: STRING, Value ['Special Ed AM Run', 'Pre-K/El']\n",
        "    - Bus_number: INTEGER, Unique, Range [100, 199]\n",
        "    - Bus_make: STRING\n",
        "    - Bus_model: STRING\n",
        "    - Maintenac_date: STRING, Pattern 'XX/XX/202X'\n",
        "    - Bus_capacity: INTEGER, Range [48, 60]\n",
        "    - License_plate: STRING, Unique, Pattern 'LXXXXX'\n",
        "    - Driver_ID: STRING, Unique\n",
        "    - Route_ID: STRING, Unique\n",
        "    - Route_number: INTEGER, Range [100, 130]\n",
        "    - Reason: STRING, Value ['Heavy Traffic', 'Won't Start', 'Flat Tire', 'Heavy Traffic', 'Other']\n",
        "    - Schools_serviced: STRING\n",
        "    - Bus_created_on: STRING, Pattern 'XX/XX/202X'\n",
        "    - How_long_delayed: STRING, Pattern 'XX MINS'\n",
        "    - Number_of_students_on_the_bus: INTEGER, Range [0, 15]\n",
        "    - Has_contractor_notified_schools: STRING, Value ['Yes', 'No']\n",
        "    - Has_contractor_notified_parents: STRING, Value ['Yes', 'No']\n",
        "    - Has_driver_alerted_OPT: STRING, Value ['Yes', 'No']\n",
        "    - Incident_occurred_On: STRING, Pattern 'XX/XX/202X'\n",
        "    - Informed_On: STRING, Pattern 'XX/XX/202X'\n",
        "    - On_schedule_rate(%): INTEGER, Range [50, 100]\n",
        "    - Driver_absence_days_per_year: INTEGER, Range [0, 10]\n",
        "\n",
        "10. Bus_data_1:\n",
        "    - Bus_ID: STRING, Unique\n",
        "    - Bus_number: INTEGER, Unique, Range [100, 199]\n",
        "    - Bus_make: STRING\n",
        "    - Bus_model: STRING\n",
        "    - Maintenac_date: STRING, Pattern 'XX/XX/202X'\n",
        "    - Bus_capacity: INTEGER, Range [48, 60]\n",
        "    - License_plate: STRING, Unique, Pattern 'LXXXXX'\n",
        "    - Fuel_type: STRING, Value ['Diesel', 'Eletric']\n",
        "    - Driver_ID: STRING, Unique\n",
        "    - First_name: STRING\n",
        "    - Last_name: STRING\n",
        "    - Email: STRING\n",
        "    - Phone_number: STRING, Unique, Pattern 'XXX-XXX-XXXX'\n",
        "    - Gender: STRING, Values ['Male', 'Female', 'Genderqueer', 'Non-binary', 'Agender', 'Genderfluid', 'Bigender', 'Polygender']\n",
        "    - Age: INTEGER, Range [35, 65]\n",
        "    - Experience_years: INTEGER, Range [1, 15]\n",
        "    - On_schedule_rate(%): INTEGER, Range [70, 100]\n",
        "    - Driver_absence_days_per_year: INTEGER, Range [0, 10]\n",
        "\n",
        "Relationships:\n",
        "- A Bus can be assigned to multiple Routes (many-to-many through BusAssignments).\n",
        "- A Route can have multiple Buses assigned to it (many-to-many through BusAssignments).\n",
        "- A Driver can drive multiple Routes (one-to-many through Schedules).\n",
        "- A Route can be driven by multiple Drivers (one-to-many through Schedules).\n",
        "- A Schedule links a Route, a Driver, and a Bus (many-to-many through Schedules).\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aG8cOJEsUJ7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User版本"
      ],
      "metadata": {
        "id": "rB4wq3Me5f3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Main_bot:\n",
        "    def __init__(self, testing, model=MODEL, api_key=openai.api_key, computations_supported=computations_supported, schema=schema, dataframes=dataframes, user_id=None, folder_path=None):\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "        self.model = model\n",
        "        self.testing = testing\n",
        "        self.computations_supported = computations_supported\n",
        "        self.dataframes = dataframes\n",
        "        self.schema = schema\n",
        "        self.user_id = user_id\n",
        "        self.folder_path = folder_path\n",
        "\n",
        "        self.functions_called = []\n",
        "        self.function_used_count = {}\n",
        "        self.function_sequences = []\n",
        "\n",
        "        self.conversation_history = []\n",
        "        self.complete_conversation_history = []\n",
        "        self.initial_system_description = '''You are a chatbot system designed to take user input and allow the assistant to chat with the user.\n",
        "        Meanwhile, the system constantly generates suggestions on what to do next before the assistant produces any output.\n",
        "        After receiving user input, the system will generate a list of the most likely functions needed to continue the conversation.\n",
        "        For example, it might suggest functions such as [{\"function_name\": \"function1\", \"params\": {\"columns\": [[\"table2\", \"col1\"], [\"table5\", \"col3\"]], \"output_table_name\": \"new_table\", \"output_column_names\": [\"col1_new\", \"col2_new\"]}, \"probability\": \"90\"}, ...].\n",
        "        Ensure that the column input exists in the specified table. Pay close attention to the schema to avoid any mistakes.\n",
        "        The output will be a JSON-formatted list of dictionaries, each containing a function name, its parameters, a probability score, an output table name, and output column names.\n",
        "        The computations will be performed as instructed, and the results will be provided to the system through the system role.\n",
        "        The assistant will then use this information to generate a direct response to the user's request, focusing only on the insightful results.\n",
        "        If a computation involves multiple steps, each step will output a new table, and subsequent steps can use these tables as part of the dataframes or database.\n",
        "        For example, converting numerical age to categorical and then performing classification on the new categorical age column, with filters applied to select specific categories before classification.'''\n",
        "\n",
        "        self.system_instruction_reminder = '''\\nTask description:\n",
        "        Generate a dict-like output mapping function names and their parameters to probabilities as described before.\n",
        "        Include an \"output_table_name\" key and an \"output_column_names\" key within the params.\n",
        "        Decide the probability based on previous conversations and the most recent user's request.\n",
        "        No matter what the user input is, your job is always the same; you are not supposed to respond freely.\n",
        "        Ensure the dict-like output is the only content so it can be easily jsonified. The output format is always:\n",
        "        [{\"function_name\": \"function1\", \"params\": {\"columns\": [[\"table2\", \"col1\"], [\"table5\", \"col3\"]], \"output_table_name\": \"new_table\", \"output_column_names\": [\"col1_new\", \"col2_new\"]}, \"probability\": \"90\"}, ...].\n",
        "        For example: [{\"function_name\": \"count\", \"params\": {\"columns\": [[\"Drivers\", \"Driver_ID\"]], \"output_table_name\": \"count_table\", \"output_column_names\": [\"Driver_ID_count\"]}, \"probability\": \"95\"}].\n",
        "        Remember to enclose everything in a pair of [ ]. When there are no valuable functions to be called, return an empty list.\n",
        "        Ensure the column input exists in the specified table. Pay close attention to the schema to avoid any mistakes.\n",
        "        Include \"filters\" when necessary to refine the data before performing computations. Filters should be in the format: \"filters\": [{\"table\": \"table1\", \"col\": \"col1\", \"range\": [\"category1\", \"category3\"], \"exclude\": true}, {\"table\": \"table2\", \"col\": \"col7\", \"range\": [[0, 9], [13, 90]], \"exclude\": false}]. In each range list, one value can be INF or NINF indicating infinity and negative infinity.\n",
        "        The order of the column names in \"output_column_names\" should match the order of the columns processed as specified in the \"columns\" input.\n",
        "        Make sure that the inputs comes from the schema I gave you or output tables in previous function'''\n",
        "\n",
        "        self.chat_assistant_reminder = '''\\nTask description:\n",
        "        Based on the user's last request and the computation results provided to you, come up with an insightful report answering the user's question or continue the conversation if there was no obvious question.\n",
        "        If no computation result is provided, then just chat with the user as you like.\n",
        "        When multiple results are provided, identify the most useful information and present it in your own way.\n",
        "        Whenever you feel it's necessary to include a graph in your answer, use the format ||chartjs: ...||, where ... contains the Chart.js configuration in JSON format.\n",
        "        You only need to specify the data and type in the Chart.js configuration; additional parameters like background color are not necessary.\n",
        "        In your text, never explicitly cite or mention the graph you might use because if the graphing steps on the backend fail, your output will remain coherent.\n",
        "        Be sure to use graph more often when talking about data.\n",
        "        '''\n",
        "\n",
        "        self.intial_assistant_request = \"Hello, I am here to help you to learn about your database, provide you with interesting and helpful insights from your data\"\n",
        "\n",
        "        self.first_run = True\n",
        "        self.log_conversation('system', \"\\nThis is a description of the database I have:\\n\" + schema)\n",
        "        self.log_conversation(\n",
        "            'system',\n",
        "            \"\\nHere's all the possible computations or functions that you might need to use:\\n\" +\n",
        "            \"\\n\".join([f\"{key}: {value['description']}\" for key, value in self.computations_supported.items()]))\n",
        "        self.log_conversation('system', self.initial_system_description)\n",
        "        self.assistant_output = self.intial_assistant_request\n",
        "\n",
        "    def log_conversation(self, role, content):  # role range assistant system user\n",
        "        self.conversation_history.append({'role': role, 'content': content})\n",
        "        self.complete_conversation_history.append({'role': role, 'content': content})\n",
        "\n",
        "    def generate(self, use_complete_log=False):\n",
        "        if use_complete_log:\n",
        "            response = self.client.chat.completions.create(\n",
        "              messages=self.complete_conversation_history,\n",
        "              model=self.model,\n",
        "              temperature=0\n",
        "          )\n",
        "\n",
        "        # Call the AI model to suggest visualization based on the data_json\n",
        "        else:\n",
        "            response = self.client.chat.completions.create(\n",
        "              messages=self.conversation_history,\n",
        "              model=self.model,\n",
        "              temperature=0\n",
        "          )\n",
        "\n",
        "        # Get the model's response and log it\n",
        "        visualization_response = response.choices[0].message.content.strip()\n",
        "        return visualization_response\n",
        "\n",
        "    def filter_and_rank_functions(self, response):\n",
        "        # Strip everything before the first `[` and after the last `]`\n",
        "        start = response.find('[')\n",
        "        end = response.rfind(']') + 1\n",
        "        if start == -1 or end == -1:\n",
        "            raise ValueError(\"No valid JSON array found in response.\")\n",
        "\n",
        "        json_text = response[start:end]\n",
        "        function_probs = json.loads(json_text)\n",
        "        return function_probs\n",
        "\n",
        "    def get_computation_results(self, computation_to_use_rank_dict_txt_response):\n",
        "        # Get the ranked functions to use\n",
        "        computation_to_use_rank_dict = self.filter_and_rank_functions(computation_to_use_rank_dict_txt_response)\n",
        "        tem_dataframes = self.dataframes.copy()\n",
        "        results = []\n",
        "\n",
        "        for item in computation_to_use_rank_dict:\n",
        "            computation_name = item[\"function_name\"]\n",
        "            params = item[\"params\"]\n",
        "            output_table_name = params.get(\"output_table_name\", None)\n",
        "\n",
        "            if computation_name in self.computations_supported:\n",
        "                function = self.computations_supported[computation_name]['function']\n",
        "                self.function_used_count[computation_name] = self.function_used_count.get(computation_name, 0) + 1\n",
        "\n",
        "                if self.testing:\n",
        "                    print(\"Currently processing:\" + computation_name)\n",
        "                    # Perform the computation\n",
        "                    result = function(tem_dataframes, params)\n",
        "                    print(result)\n",
        "                    if output_table_name:\n",
        "                        tem_dataframes[output_table_name] = result\n",
        "                        results.append(f\"{computation_name} ({params}):\\n{result.to_string(index=False)}\\n\")\n",
        "                    else:\n",
        "                        results.append(f\"{computation_name} ({params}):\\n{result.to_string(index=False)}\\n\")\n",
        "                else:\n",
        "                    try:\n",
        "                        # Perform the computation\n",
        "                        result = function(tem_dataframes, params)\n",
        "                        self.functions_called.append({\n",
        "                            'function': computation_name,\n",
        "                            'error': False,\n",
        "                            'inputs': params\n",
        "                        })\n",
        "                        if output_table_name:\n",
        "                            tem_dataframes[output_table_name] = result\n",
        "                            results.append(f\"{computation_name} ({params}):\\n{result.to_string(index=False)}\\n\")\n",
        "                        else:\n",
        "                            results.append(f\"{computation_name} ({params}):\\n{result.to_string(index=False)}\\n\")\n",
        "                    except Exception as e:\n",
        "                        # Log the error details\n",
        "                        self.functions_called.append({\n",
        "                            'function': computation_name,\n",
        "                            'error': True,\n",
        "                            'inputs': params\n",
        "                        })\n",
        "                        # Do not append error details to results in non-testing mode\n",
        "                        pass\n",
        "        # Join all results into a single string\n",
        "        computation_results_str = \"\\n\".join(results)\n",
        "\n",
        "        if not results:\n",
        "            return \"All given functions are facing errors, please respond freely as you see fit.\"\n",
        "\n",
        "        return computation_results_str\n",
        "\n",
        "    def generate_and_display_chart(self, config, width=500, height=300, version='2.9.4'):\n",
        "        qc = QuickChart()\n",
        "        qc.width = width\n",
        "        qc.height = height\n",
        "        qc.version = version\n",
        "        qc.config = config\n",
        "\n",
        "        # Fetch the image as bytes directly from QuickChart\n",
        "        image_data = qc.get_bytes()\n",
        "\n",
        "        # Display the image in the notebook\n",
        "        display(Image(image_data))\n",
        "\n",
        "    def process_output(self):\n",
        "        input_string = self.assistant_output\n",
        "        # Split the input string by the graph indicator\n",
        "        chunks = input_string.split('||')\n",
        "\n",
        "        for chunk in chunks:\n",
        "            if chunk.startswith('chartjs:'):\n",
        "                # Extract the JSON part of the chunk\n",
        "                json_part = chunk[len('chartjs:'):]\n",
        "\n",
        "                try:\n",
        "                    # Convert the JSON part to a dictionary\n",
        "                    config = json.loads(json_part)\n",
        "                    # Generate and display the chart\n",
        "                    self.generate_and_display_chart(config)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Error decoding JSON: {e}\")\n",
        "            else:\n",
        "                # Print the text chunk\n",
        "                print(chunk)\n",
        "\n",
        "\n",
        "\n",
        "    def generate_function_sequences(self):\n",
        "        sequences = []\n",
        "        current_sequence = []\n",
        "        error_occur = None\n",
        "\n",
        "        for func in self.functions_called:\n",
        "            current_sequence.append(func['function'])\n",
        "            if func['error'] and not error_occur:\n",
        "                error_occur = func['function']\n",
        "\n",
        "        sequences.append({\n",
        "            'sequence': ' -> '.join(current_sequence),\n",
        "            'error_occur': error_occur if error_occur else 'None'\n",
        "        })\n",
        "\n",
        "        self.function_sequences = sequences\n",
        "\n",
        "    def execute(self):\n",
        "        while True:\n",
        "            # get user request and log it\n",
        "            self.log_conversation('assistant', self.assistant_output)\n",
        "            self.process_output()\n",
        "            print('\\n')\n",
        "            user_request = input('\\n')\n",
        "            self.log_conversation('user', user_request)\n",
        "\n",
        "            # system tell which function to use, don't log this step\n",
        "            self.log_conversation('system', self.system_instruction_reminder)\n",
        "            computation_to_use_rank_dict_txt_response = self.generate()  # text json-like\n",
        "\n",
        "            if self.testing:\n",
        "                print(\"computation_to_use_rank_dict_txt_response:\", computation_to_use_rank_dict_txt_response)\n",
        "\n",
        "            computation_results = self.get_computation_results(computation_to_use_rank_dict_txt_response)\n",
        "\n",
        "            # system provide computation results, and remind assistant's task, to generate report\n",
        "            self.log_conversation('system', computation_results)\n",
        "            self.log_conversation('system', self.chat_assistant_reminder)\n",
        "\n",
        "            # assistant output report using given data\n",
        "            self.assistant_output = self.generate()\n",
        "\n",
        "            self.conversation_history.pop()\n",
        "            self.conversation_history.pop()\n",
        "            print(\"\\n\\n\\n\\n\\n\")\n",
        "    def record(self):\n",
        "        # Ensure the directory exists\n",
        "        logs_directory = os.path.join(self.folder_path, self.user_id, 'logs')\n",
        "        if not os.path.exists(logs_directory):\n",
        "            os.makedirs(logs_directory)\n",
        "\n",
        "        # Get the current timestamp for unique filenames\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Save complete conversation history\n",
        "        conversation_file = os.path.join(logs_directory, f'log_{timestamp}.json')\n",
        "        with open(conversation_file, 'w') as file:\n",
        "            json.dump(self.complete_conversation_history, file, indent=4)\n",
        "\n",
        "        # Save function sequences\n",
        "        function_sequences_file = os.path.join(logs_directory, f'functions_{timestamp}.json')\n",
        "        with open(function_sequences_file, 'w') as file:\n",
        "            json.dump(self.function_sequences, file, indent=4)\n",
        "\n",
        "        # File paths for user-specific CSVs\n",
        "        user_folder_path = os.path.join(self.folder_path, self.user_id)\n",
        "        functions_called_file_user = os.path.join(user_folder_path, 'functions_called.csv')\n",
        "        functions_count_file_user = os.path.join(user_folder_path, 'functions_count.csv')\n",
        "        function_sequences_file_user = os.path.join(user_folder_path, 'function_sequences.csv')\n",
        "\n",
        "        # File paths for global CSVs\n",
        "        functions_called_file_global = os.path.join(self.folder_path, 'functions_called.csv')\n",
        "        functions_count_file_global = os.path.join(self.folder_path, 'functions_count.csv')\n",
        "        function_sequences_file_global = os.path.join(self.folder_path, 'function_sequences.csv')\n",
        "\n",
        "        # Helper function to load existing data\n",
        "        def load_existing_data(file_path, fieldnames):\n",
        "            data = []\n",
        "            if os.path.exists(file_path):\n",
        "                with open(file_path, mode='r', newline='') as file:\n",
        "                    reader = csv.DictReader(file, fieldnames=fieldnames)\n",
        "                    next(reader)  # Skip header\n",
        "                    data = list(reader)\n",
        "            return data\n",
        "\n",
        "        # Helper function to save data\n",
        "        def save_data(file_path, data, fieldnames):\n",
        "            with open(file_path, mode='w', newline='') as file:\n",
        "                writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                for item in data:\n",
        "                    writer.writerow(item)\n",
        "\n",
        "        # Load existing function counts (user-specific and global)\n",
        "        function_counts_user = load_existing_data(functions_count_file_user, ['function', 'count'])\n",
        "        function_counts_global = load_existing_data(functions_count_file_global, ['function', 'count'])\n",
        "\n",
        "        # Save or update function counts (user-specific and global)\n",
        "        for function_name, count in self.function_used_count.items():\n",
        "            found_user = False\n",
        "            for item in function_counts_user:\n",
        "                if item['function'] == function_name:\n",
        "                    item['count'] = int(item['count']) + count\n",
        "                    found_user = True\n",
        "                    break\n",
        "            if not found_user:\n",
        "                function_counts_user.append({'function': function_name, 'count': count})\n",
        "\n",
        "            found_global = False\n",
        "            for item in function_counts_global:\n",
        "                if item['function'] == function_name:\n",
        "                    item['count'] = int(item['count']) + count\n",
        "                    found_global = True\n",
        "                    break\n",
        "            if not found_global:\n",
        "                function_counts_global.append({'function': function_name, 'count': count})\n",
        "\n",
        "        save_data(functions_count_file_user, function_counts_user, ['function', 'count'])\n",
        "        save_data(functions_count_file_global, function_counts_global, ['function', 'count'])\n",
        "\n",
        "        # Load existing functions_called (user-specific and global)\n",
        "        functions_called_user = load_existing_data(functions_called_file_user, ['function', 'error', 'inputs', 'count'])\n",
        "        functions_called_global = load_existing_data(functions_called_file_global, ['function', 'error', 'inputs', 'count'])\n",
        "\n",
        "        # Save or update functions_called (user-specific and global)\n",
        "        def update_functions_called(existing_functions, new_functions):\n",
        "            for func in new_functions:\n",
        "                found = False\n",
        "                for existing_func in existing_functions:\n",
        "                    if existing_func['function'] == func['function'] and existing_func['inputs'] == func['inputs']:\n",
        "                        existing_func['error'] = func['error']\n",
        "                        existing_func['count'] = int(existing_func['count']) + 1\n",
        "                        found = True\n",
        "                        break\n",
        "                if not found:\n",
        "                    func['count'] = 1\n",
        "                    existing_functions.append(func)\n",
        "            return existing_functions\n",
        "\n",
        "        functions_called_user = update_functions_called(functions_called_user, self.functions_called)\n",
        "        functions_called_global = update_functions_called(functions_called_global, self.functions_called)\n",
        "\n",
        "        save_data(functions_called_file_user, functions_called_user, ['function', 'error', 'inputs', 'count'])\n",
        "        save_data(functions_called_file_global, functions_called_global, ['function', 'error', 'inputs', 'count'])\n",
        "\n",
        "        # Load existing function sequences (user-specific and global)\n",
        "        function_sequences_user = load_existing_data(function_sequences_file_user, ['sequence', 'error_occur', 'count'])\n",
        "        function_sequences_global = load_existing_data(function_sequences_file_global, ['sequence', 'error_occur', 'count'])\n",
        "\n",
        "        # Generate and save function sequences (user-specific and global)\n",
        "        self.generate_function_sequences()\n",
        "\n",
        "        def update_function_sequences(existing_sequences, new_sequences):\n",
        "            for seq in new_sequences:\n",
        "                found = False\n",
        "                for existing_seq in existing_sequences:\n",
        "                    if existing_seq['sequence'] == seq['sequence']:\n",
        "                        existing_seq['error_occur'] = seq['error_occur']\n",
        "                        existing_seq['count'] = int(existing_seq['count']) + 1\n",
        "                        found = True\n",
        "                        break\n",
        "                if not found:\n",
        "                    seq['count'] = 1\n",
        "                    existing_sequences.append(seq)\n",
        "            return existing_sequences\n",
        "\n",
        "        function_sequences_user = update_function_sequences(function_sequences_user, self.function_sequences)\n",
        "        function_sequences_global = update_function_sequences(function_sequences_global, self.function_sequences)\n",
        "\n",
        "        save_data(function_sequences_file_user, function_sequences_user, ['sequence', 'error_occur', 'count'])\n",
        "        save_data(function_sequences_file_global, function_sequences_global, ['sequence', 'error_occur', 'count'])\n",
        "\n",
        "    def generate_summary_report(self):\n",
        "        # Generate a summary report from the complete conversation history\n",
        "        summary_prompt = '''Generate a summarizing report of the conversation history. Include insights and graphs where relevant using the ||chartjs: ...|| format for graphs.'''\n",
        "        self.log_conversation('system', summary_prompt)\n",
        "\n",
        "        # Generate the summary using the complete conversation history\n",
        "        summary_response = self.generate()\n",
        "\n",
        "        # Split the summary response by the graph indicator\n",
        "        summary_chunks = summary_response.split('||')\n",
        "\n",
        "        # Display the summary with graphs\n",
        "        # self.display_summary_with_graphs(summary_chunks)\n",
        "\n",
        "        # Ensure the logs directory exists\n",
        "        logs_directory = os.path.join(self.folder_path, self.user_id, 'logs')\n",
        "        if not os.path.exists(logs_directory):\n",
        "            os.makedirs(logs_directory)\n",
        "\n",
        "        # Save the summary response as a text file\n",
        "        summary_file = os.path.join(logs_directory, f'summary_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt')\n",
        "        with open(summary_file, 'w') as file:\n",
        "            file.write(summary_response)\n",
        "\n",
        "    def display_summary_with_graphs(self, summary_chunks):\n",
        "        for chunk in summary_chunks:\n",
        "            if chunk.startswith('chartjs:'):\n",
        "                json_part = chunk[len('chartjs:'):]\n",
        "                try:\n",
        "                    config = json.loads(json_part)\n",
        "                    qc = QuickChart()\n",
        "                    qc.config = config\n",
        "                    image_url = qc.get_short_url()\n",
        "                    display(Image(image_url))\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Error decoding JSON: {e}\")\n",
        "            else:\n",
        "                print(chunk)\n",
        "\n",
        "    def check_and_update_required_functions(self, show=False):\n",
        "        # Ensure the directory exists\n",
        "        outermost_directory = self.folder_path\n",
        "        required_functions_file = os.path.join(outermost_directory, 'required_functions.csv')\n",
        "\n",
        "        # Read existing functions if the file exists\n",
        "        existing_functions = []\n",
        "        if os.path.exists(required_functions_file):\n",
        "            with open(required_functions_file, mode='r', newline='') as file:\n",
        "                reader = csv.DictReader(file)\n",
        "                existing_functions = list(reader)\n",
        "\n",
        "        # Ask the system to identify required functions based on the complete log history\n",
        "        required_functions_prompt = '''Based on the complete conversation history, identify any specific user requests that are not supported by the current set of functions or their combinations. Only include the functions that are not already supported. Provide the names and descriptions of the required functions in the following format: [{\"function_name\": \"example_function_name\", \"description\": \"This is an example description of the function.\"}, ...]. If a required function is very similar to any existing function descriptions, copy the existing function name and description.'''\n",
        "        self.log_conversation('system', required_functions_prompt)\n",
        "\n",
        "        required_functions_response = self.generate()\n",
        "        if show:\n",
        "            print(required_functions_response)\n",
        "        try:\n",
        "            new_required_functions = json.loads(required_functions_response)\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Error decoding JSON response for required functions.\")\n",
        "            return\n",
        "\n",
        "        # Update existing functions with new required functions\n",
        "        for new_function in new_required_functions:\n",
        "            found = False\n",
        "            for existing_function in existing_functions:\n",
        "                if (existing_function['function_name'] == new_function['function_name'] and\n",
        "                        existing_function['description'] == new_function['description']):\n",
        "                    existing_function['count'] = int(existing_function['count']) + 1\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                new_function['count'] = 1\n",
        "                existing_functions.append(new_function)\n",
        "\n",
        "        # Save updated required functions back to the file\n",
        "        with open(required_functions_file, mode='w', newline='') as file:\n",
        "            fieldnames = ['function_name', 'description', 'count']\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for func in existing_functions:\n",
        "                writer.writerow(func)\n",
        "\n",
        "\n",
        "    def print_conversation(self):\n",
        "        for entry in self.conversation_history:\n",
        "            print(f\"{entry['role'].capitalize()}: {entry['content']}\")\n",
        "\n",
        "    def print_complete_conversation(self):\n",
        "        for entry in self.complete_conversation_history:\n",
        "            print(f\"{entry['role'].capitalize()}: {entry['content']}\")\n",
        "\n",
        "chat = Main_bot(testing=False, user_id='user123', folder_path=base_path+'saved_functions_log/')\n",
        "chat.execute()\n"
      ],
      "metadata": {
        "id": "7nbEX2ir5oHk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "62e59adc-c07a-4c3f-e7e3-d18b1b9d4a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, I am here to help you to learn about your database, provide you with interesting and helpful insights from your data\n",
            "\n",
            "\n",
            "\n",
            "scscasca\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: xxx # us****************** key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-51908515a58d>\u001b[0m in \u001b[0;36m<cell line: 458>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0mchat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMain_bot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'user123'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'saved_functions_log/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-51908515a58d>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# system tell which function to use, don't log this step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_conversation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'system'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem_instruction_reminder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mcomputation_to_use_rank_dict_txt_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# text json-like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-51908515a58d>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, use_complete_log)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Call the AI model to suggest visualization based on the data_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             response = self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m     79\u001b[0m               \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconversation_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m               \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 646\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    647\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m         )\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 942\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    943\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: xxx # us****************** key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Call the record method to save the logs and function sequences\n",
        "chat.record()\n",
        "\n",
        "# Generate and print the summary report with graphs\n",
        "chat.generate_summary_report()\n",
        "\n",
        "# Check and update required functions based on the complete log history\n",
        "chat.check_and_update_required_functions(True)\n"
      ],
      "metadata": {
        "id": "GkbZpkEuEALY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}